{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Hierarchical Indices for RAG\n",
    "\n",
    "In this notebook, I implement a hierarchical indexing approach for RAG systems. This technique improves retrieval by using a two-tier search method: first identifying relevant document sections through summaries, then retrieving specific details from those sections.\n",
    "\n",
    "Traditional RAG approaches treat all text chunks equally, which can lead to:\n",
    "\n",
    "- Lost context when chunks are too small\n",
    "- Irrelevant results when the document collection is large\n",
    "- Inefficient searches across the entire corpus\n",
    "\n",
    "Hierarchical retrieval solves these problems by:\n",
    "\n",
    "- Creating concise summaries for larger document sections\n",
    "- First searching these summaries to identify relevant sections\n",
    "- Then retrieving detailed information only from those sections\n",
    "- Maintaining context while preserving specific details\n",
    "# 层次化索引在RAG中的应用\n",
    "\n",
    "在本笔记本中，我实现了一种用于RAG系统的层次化索引方法。该技术通过使用两级搜索方法来改进检索：首先通过摘要识别相关文档部分，然后从这些部分中检索具体细节。\n",
    "\n",
    "传统的RAG方法将所有文本块同等对待，这可能导致：\n",
    "\n",
    "- 当文本块过小时丢失上下文\n",
    "- 文档集合较大时出现不相关结果\n",
    "- 在整个语料库中进行低效搜索\n",
    "\n",
    "层次化检索通过以下方式解决这些问题：\n",
    "\n",
    "- 为较大的文档部分创建简洁摘要\n",
    "- 首先搜索这些摘要以识别相关部分\n",
    "- 然后仅从这些部分检索详细信息\n",
    "- 在保留具体细节的同时维护上下文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries.\n",
    "## 设置环境\n",
    "我们首先导入必要的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses.\n",
    "## 设置OpenAI API客户端\n",
    "我们初始化OpenAI客户端，用于生成嵌入向量和响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"xxx\"\n",
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Functions\n",
    "## 文档处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF file with page separation.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of pages with text content and metadata\n",
    "    \"\"\"\n",
    "    print(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\n",
    "    pdf = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\n",
    "    pages = []  # Initialize an empty list to store the pages with text content\n",
    "    \n",
    "    # Iterate over each page in the PDF\n",
    "    for page_num in range(len(pdf)):\n",
    "        page = pdf[page_num]  # Get the current page\n",
    "        text = page.get_text()  # Extract text from the current page\n",
    "        \n",
    "        # Skip pages with very little text (less than 50 characters)\n",
    "        if len(text.strip()) > 50:\n",
    "            # Append the page text and metadata to the list\n",
    "            pages.append({\n",
    "                \"text\": text,\n",
    "                \"metadata\": {\n",
    "                    \"source\": pdf_path,  # Source file path\n",
    "                    \"page\": page_num + 1  # Page number (1-based index)\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    print(f\"Extracted {len(pages)} pages with content\")  # Print the number of pages extracted\n",
    "    return pages  # Return the list of pages with text content and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, metadata, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks while preserving metadata.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        metadata (Dict): Metadata to preserve\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of text chunks with metadata\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Iterate over the text with the specified chunk size and overlap\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk_text = text[i:i + chunk_size]  # Extract the chunk of text\n",
    "        \n",
    "        # Skip very small chunks (less than 50 characters)\n",
    "        if chunk_text and len(chunk_text.strip()) > 50:\n",
    "            # Create a copy of metadata and add chunk-specific info\n",
    "            chunk_metadata = metadata.copy()\n",
    "            chunk_metadata.update({\n",
    "                \"chunk_index\": len(chunks),  # Index of the chunk\n",
    "                \"start_char\": i,  # Start character index of the chunk\n",
    "                \"end_char\": i + len(chunk_text),  # End character index of the chunk\n",
    "                \"is_summary\": False  # Flag indicating this is not a summary\n",
    "            })\n",
    "            \n",
    "            # Append the chunk with its metadata to the list\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"metadata\": chunk_metadata\n",
    "            })\n",
    "    \n",
    "    return chunks  # Return the list of chunks with metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store Implementation\n",
    "## 简单向量存储实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []  # List to store vector embeddings\n",
    "        self.texts = []  # List to store text content\n",
    "        self.metadata = []  # List to store metadata\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text content\n",
    "            embedding (List[float]): Vector embedding\n",
    "            metadata (Dict, optional): Additional metadata\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Append the embedding as a numpy array\n",
    "        self.texts.append(text)  # Append the text content\n",
    "        self.metadata.append(metadata or {})  # Append the metadata or an empty dict if None\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): Query embedding vector\n",
    "            k (int): Number of results to return\n",
    "            filter_func (callable, optional): Function to filter results\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Top k most similar items\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return an empty list if there are no vectors\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Skip if doesn't pass the filter\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "                \n",
    "            # Calculate cosine similarity\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Add the text content\n",
    "                \"metadata\": self.metadata[idx],  # Add the metadata\n",
    "                \"similarity\": float(score)  # Add the similarity score\n",
    "            })\n",
    "        \n",
    "        return results  # Return the list of top k results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings\n",
    "## 创建嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    Create embeddings for the given texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): Input texts\n",
    "        model (str): Embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: Embedding vectors\n",
    "    \"\"\"\n",
    "    # Handle empty input\n",
    "    if not texts:\n",
    "        return []\n",
    "        \n",
    "    # Process in batches if needed (OpenAI API limits)\n",
    "    batch_size = 100\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Iterate over the input texts in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]  # Get the current batch of texts\n",
    "        \n",
    "        # Create embeddings for the current batch\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        # Extract embeddings from the response\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n",
    "    \n",
    "    return all_embeddings  # Return all embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization Function\n",
    "## 摘要函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_page_summary(page_text):\n",
    "    \"\"\"\n",
    "    Generate a concise summary of a page.\n",
    "    \n",
    "    Args:\n",
    "        page_text (str): Text content of the page\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated summary\n",
    "    \"\"\"\n",
    "    # Define the system prompt to instruct the summarization model\n",
    "    system_prompt = \"\"\"You are an expert summarization system.\n",
    "    Create a detailed summary of the provided text. \n",
    "    Focus on capturing the main topics, key information, and important facts.\n",
    "    Your summary should be comprehensive enough to understand what the page contains\n",
    "    but more concise than the original.\"\"\"\n",
    "\n",
    "    # Truncate input text if it exceeds the maximum token limit\n",
    "    max_tokens = 6000\n",
    "    truncated_text = page_text[:max_tokens] if len(page_text) > max_tokens else page_text\n",
    "\n",
    "    # Make a request to the OpenAI API to generate the summary\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"google/gemma-2-2b-it\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": f\"Please summarize this text:\\n\\n{truncated_text}\"}  # User message with the text to summarize\n",
    "        ],\n",
    "        temperature=0.3  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated summary content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Document Processing\n",
    "## 分层文档处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_hierarchically(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document into hierarchical indices.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each detailed chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[SimpleVectorStore, SimpleVectorStore]: Summary and detailed vector stores\n",
    "    \"\"\"\n",
    "    # Extract pages from PDF\n",
    "    pages = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Create summaries for each page\n",
    "    print(\"Generating page summaries...\")\n",
    "    summaries = []\n",
    "    for i, page in enumerate(pages):\n",
    "        print(f\"Summarizing page {i+1}/{len(pages)}...\")\n",
    "        summary_text = generate_page_summary(page[\"text\"])\n",
    "        \n",
    "        # Create summary metadata\n",
    "        summary_metadata = page[\"metadata\"].copy()\n",
    "        summary_metadata.update({\"is_summary\": True})\n",
    "        \n",
    "        # Append the summary text and metadata to the summaries list\n",
    "        summaries.append({\n",
    "            \"text\": summary_text,\n",
    "            \"metadata\": summary_metadata\n",
    "        })\n",
    "    \n",
    "    # Create detailed chunks for each page\n",
    "    detailed_chunks = []\n",
    "    for page in pages:\n",
    "        # Chunk the text of the page\n",
    "        page_chunks = chunk_text(\n",
    "            page[\"text\"], \n",
    "            page[\"metadata\"], \n",
    "            chunk_size, \n",
    "            chunk_overlap\n",
    "        )\n",
    "        # Extend the detailed_chunks list with the chunks from the current page\n",
    "        detailed_chunks.extend(page_chunks)\n",
    "    \n",
    "    print(f\"Created {len(detailed_chunks)} detailed chunks\")\n",
    "    \n",
    "    # Create embeddings for summaries\n",
    "    print(\"Creating embeddings for summaries...\")\n",
    "    summary_texts = [summary[\"text\"] for summary in summaries]\n",
    "    summary_embeddings = create_embeddings(summary_texts)\n",
    "    \n",
    "    # Create embeddings for detailed chunks\n",
    "    print(\"Creating embeddings for detailed chunks...\")\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in detailed_chunks]\n",
    "    chunk_embeddings = create_embeddings(chunk_texts)\n",
    "    \n",
    "    # Create vector stores\n",
    "    summary_store = SimpleVectorStore()\n",
    "    detailed_store = SimpleVectorStore()\n",
    "    \n",
    "    # Add summaries to summary store\n",
    "    for i, summary in enumerate(summaries):\n",
    "        summary_store.add_item(\n",
    "            text=summary[\"text\"],\n",
    "            embedding=summary_embeddings[i],\n",
    "            metadata=summary[\"metadata\"]\n",
    "        )\n",
    "    \n",
    "    # Add chunks to detailed store\n",
    "    for i, chunk in enumerate(detailed_chunks):\n",
    "        detailed_store.add_item(\n",
    "            text=chunk[\"text\"],\n",
    "            embedding=chunk_embeddings[i],\n",
    "            metadata=chunk[\"metadata\"]\n",
    "        )\n",
    "    \n",
    "    print(f\"Created vector stores with {len(summaries)} summaries and {len(detailed_chunks)} chunks\")\n",
    "    return summary_store, detailed_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Retrieval\n",
    "## 分层检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hierarchically(query, summary_store, detailed_store, k_summaries=3, k_chunks=5):\n",
    "    \"\"\"\n",
    "    Retrieve information using hierarchical indices.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        summary_store (SimpleVectorStore): Store of document summaries\n",
    "        detailed_store (SimpleVectorStore): Store of detailed chunks\n",
    "        k_summaries (int): Number of summaries to retrieve\n",
    "        k_chunks (int): Number of chunks to retrieve per summary\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved chunks with relevance scores\n",
    "    \"\"\"\n",
    "    print(f\"Performing hierarchical retrieval for query: {query}\")\n",
    "    \n",
    "    # Create query embedding\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # First, retrieve relevant summaries\n",
    "    summary_results = summary_store.similarity_search(\n",
    "        query_embedding, \n",
    "        k=k_summaries\n",
    "    )\n",
    "    \n",
    "    print(f\"Retrieved {len(summary_results)} relevant summaries\")\n",
    "    \n",
    "    # Collect pages from relevant summaries\n",
    "    relevant_pages = [result[\"metadata\"][\"page\"] for result in summary_results]\n",
    "    \n",
    "    # Create a filter function to only keep chunks from relevant pages\n",
    "    def page_filter(metadata):\n",
    "        return metadata[\"page\"] in relevant_pages\n",
    "    \n",
    "    # Then, retrieve detailed chunks from only those relevant pages\n",
    "    detailed_results = detailed_store.similarity_search(\n",
    "        query_embedding, \n",
    "        k=k_chunks * len(relevant_pages),\n",
    "        filter_func=page_filter\n",
    "    )\n",
    "    \n",
    "    print(f\"Retrieved {len(detailed_results)} detailed chunks from relevant pages\")\n",
    "    \n",
    "    # For each result, add which summary/page it came from\n",
    "    for result in detailed_results:\n",
    "        page = result[\"metadata\"][\"page\"]\n",
    "        matching_summaries = [s for s in summary_results if s[\"metadata\"][\"page\"] == page]\n",
    "        if matching_summaries:\n",
    "            result[\"summary\"] = matching_summaries[0][\"text\"]\n",
    "    \n",
    "    return detailed_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation with Context\n",
    "## 基于上下文的响应生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, retrieved_chunks):\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and retrieved chunks.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        retrieved_chunks (List[Dict]): Retrieved chunks from hierarchical search\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Extract text from chunks and prepare context parts\n",
    "    context_parts = []\n",
    "    \n",
    "    for i, chunk in enumerate(retrieved_chunks):\n",
    "        page_num = chunk[\"metadata\"][\"page\"]  # Get the page number from metadata\n",
    "        context_parts.append(f\"[Page {page_num}]: {chunk['text']}\")  # Format the chunk text with page number\n",
    "    \n",
    "    # Combine all context parts into a single context string\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Define the system message to guide the AI assistant\n",
    "    system_message = \"\"\"You are a helpful AI assistant answering questions based on the provided context.\n",
    "Use the information from the context to answer the user's question accurately.\n",
    "If the context doesn't contain relevant information, acknowledge that.\n",
    "Include page numbers when referencing specific information.\"\"\"\n",
    "\n",
    "    # Generate the response using the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"google/gemma-2-2b-it\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n\\n{context}\\n\\nQuestion: {query}\"}  # User message with context and query\n",
    "        ],\n",
    "        temperature=0.2  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated response content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete RAG Pipeline with Hierarchical Retrieval\n",
    "## 带有分层检索的完整RAG（检索增强生成）流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_rag(query, pdf_path, chunk_size=1000, chunk_overlap=200, \n",
    "                    k_summaries=3, k_chunks=5, regenerate=False):\n",
    "    \"\"\"\n",
    "    Complete hierarchical RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        chunk_size (int): Size of each detailed chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        k_summaries (int): Number of summaries to retrieve\n",
    "        k_chunks (int): Number of chunks to retrieve per summary\n",
    "        regenerate (bool): Whether to regenerate vector stores\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including response and retrieved chunks\n",
    "    \"\"\"\n",
    "    # Create store filenames for caching\n",
    "    summary_store_file = f\"{os.path.basename(pdf_path)}_summary_store.pkl\"\n",
    "    detailed_store_file = f\"{os.path.basename(pdf_path)}_detailed_store.pkl\"\n",
    "    \n",
    "    # Process document and create stores if needed\n",
    "    if regenerate or not os.path.exists(summary_store_file) or not os.path.exists(detailed_store_file):\n",
    "        print(\"Processing document and creating vector stores...\")\n",
    "        # Process the document to create hierarchical indices and vector stores\n",
    "        summary_store, detailed_store = process_document_hierarchically(\n",
    "            pdf_path, chunk_size, chunk_overlap\n",
    "        )\n",
    "        \n",
    "        # Save the summary store to a file for future use\n",
    "        with open(summary_store_file, 'wb') as f:\n",
    "            pickle.dump(summary_store, f)\n",
    "        \n",
    "        # Save the detailed store to a file for future use\n",
    "        with open(detailed_store_file, 'wb') as f:\n",
    "            pickle.dump(detailed_store, f)\n",
    "    else:\n",
    "        # Load existing summary store from file\n",
    "        print(\"Loading existing vector stores...\")\n",
    "        with open(summary_store_file, 'rb') as f:\n",
    "            summary_store = pickle.load(f)\n",
    "        \n",
    "        # Load existing detailed store from file\n",
    "        with open(detailed_store_file, 'rb') as f:\n",
    "            detailed_store = pickle.load(f)\n",
    "    \n",
    "    # Retrieve relevant chunks hierarchically using the query\n",
    "    retrieved_chunks = retrieve_hierarchically(\n",
    "        query, summary_store, detailed_store, k_summaries, k_chunks\n",
    "    )\n",
    "    \n",
    "    # Generate a response based on the retrieved chunks\n",
    "    response = generate_response(query, retrieved_chunks)\n",
    "    \n",
    "    # Return results including the query, response, retrieved chunks, and counts of summaries and detailed chunks\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"retrieved_chunks\": retrieved_chunks,\n",
    "        \"summary_count\": len(summary_store.texts),\n",
    "        \"detailed_count\": len(detailed_store.texts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard (Non-Hierarchical) RAG for Comparison\n",
    "## 标准（无分层索引）RAG用于比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_rag(query, pdf_path, chunk_size=1000, chunk_overlap=200, k=15):\n",
    "    \"\"\"\n",
    "    Standard RAG pipeline without hierarchical retrieval.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        chunk_size (int): Size of each chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        k (int): Number of chunks to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including response and retrieved chunks\n",
    "    \"\"\"\n",
    "    # Extract pages from the PDF document\n",
    "    pages = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Create chunks directly from all pages\n",
    "    chunks = []\n",
    "    for page in pages:\n",
    "        # Chunk the text of the page\n",
    "        page_chunks = chunk_text(\n",
    "            page[\"text\"], \n",
    "            page[\"metadata\"], \n",
    "            chunk_size, \n",
    "            chunk_overlap\n",
    "        )\n",
    "        # Extend the chunks list with the chunks from the current page\n",
    "        chunks.extend(page_chunks)\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks for standard RAG\")\n",
    "    \n",
    "    # Create a vector store to hold the chunks\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # Create embeddings for the chunks\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    embeddings = create_embeddings(texts)\n",
    "    \n",
    "    # Add chunks to the vector store\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        store.add_item(\n",
    "            text=chunk[\"text\"],\n",
    "            embedding=embeddings[i],\n",
    "            metadata=chunk[\"metadata\"]\n",
    "        )\n",
    "    \n",
    "    # Create an embedding for the query\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # Retrieve the most relevant chunks based on the query embedding\n",
    "    retrieved_chunks = store.similarity_search(query_embedding, k=k)\n",
    "    print(f\"Retrieved {len(retrieved_chunks)} chunks with standard RAG\")\n",
    "    \n",
    "    # Generate a response based on the retrieved chunks\n",
    "    response = generate_response(query, retrieved_chunks)\n",
    "    \n",
    "    # Return the results including the query, response, and retrieved chunks\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"retrieved_chunks\": retrieved_chunks\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n",
    "## 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_approaches(query, pdf_path, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Compare hierarchical and standard RAG approaches.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        reference_answer (str, optional): Reference answer for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Comparison results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Comparing RAG approaches for query: {query} ===\")\n",
    "    \n",
    "    # Run hierarchical RAG\n",
    "    print(\"\\nRunning hierarchical RAG...\")\n",
    "    hierarchical_result = hierarchical_rag(query, pdf_path)\n",
    "    hier_response = hierarchical_result[\"response\"]\n",
    "    \n",
    "    # Run standard RAG\n",
    "    print(\"\\nRunning standard RAG...\")\n",
    "    standard_result = standard_rag(query, pdf_path)\n",
    "    std_response = standard_result[\"response\"]\n",
    "    \n",
    "    # Compare results from hierarchical and standard RAG\n",
    "    comparison = compare_responses(query, hier_response, std_response, reference_answer)\n",
    "    \n",
    "    # Return a dictionary with the comparison results\n",
    "    return {\n",
    "        \"query\": query,  # The original query\n",
    "        \"hierarchical_response\": hier_response,  # Response from hierarchical RAG\n",
    "        \"standard_response\": std_response,  # Response from standard RAG\n",
    "        \"reference_answer\": reference_answer,  # Reference answer for evaluation\n",
    "        \"comparison\": comparison,  # Comparison analysis\n",
    "        \"hierarchical_chunks_count\": len(hierarchical_result[\"retrieved_chunks\"]),  # Number of chunks retrieved by hierarchical RAG\n",
    "        \"standard_chunks_count\": len(standard_result[\"retrieved_chunks\"])  # Number of chunks retrieved by standard RAG\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query, hierarchical_response, standard_response, reference=None):\n",
    "    \"\"\"\n",
    "    Compare responses from hierarchical and standard RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        hierarchical_response (str): Response from hierarchical RAG\n",
    "        standard_response (str): Response from standard RAG\n",
    "        reference (str, optional): Reference answer\n",
    "        \n",
    "    Returns:\n",
    "        str: Comparison analysis\n",
    "    \"\"\"\n",
    "    # Define the system prompt to instruct the model on how to evaluate the responses\n",
    "    system_prompt = \"\"\"You are an expert evaluator of information retrieval systems. \n",
    "Compare the two responses to the same query, one generated using hierarchical retrieval\n",
    "and the other using standard retrieval.\n",
    "\n",
    "Evaluate them based on:\n",
    "1. Accuracy: Which response provides more factually correct information?\n",
    "2. Comprehensiveness: Which response better covers all aspects of the query?\n",
    "3. Coherence: Which response has better logical flow and organization?\n",
    "4. Page References: Does either response make better use of page references?\n",
    "\n",
    "Be specific in your analysis of the strengths and weaknesses of each approach.\"\"\"\n",
    "\n",
    "    # Create the user prompt with the query and both responses\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "Response from Hierarchical RAG:\n",
    "{hierarchical_response}\n",
    "\n",
    "Response from Standard RAG:\n",
    "{standard_response}\"\"\"\n",
    "\n",
    "    # If a reference answer is provided, include it in the user prompt\n",
    "    if reference:\n",
    "        user_prompt += f\"\"\"\n",
    "\n",
    "Reference Answer:\n",
    "{reference}\"\"\"\n",
    "\n",
    "    # Add the final instruction to the user prompt\n",
    "    user_prompt += \"\"\"\n",
    "\n",
    "Please provide a detailed comparison of these two responses, highlighting which approach performed better and why.\"\"\"\n",
    "\n",
    "    # Make a request to the OpenAI API to generate the comparison analysis\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"google/gemma-2-2b-it\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # User message with the query and responses\n",
    "        ],\n",
    "        temperature=0  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated comparison analysis\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Run a complete evaluation with multiple test queries.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        test_queries (List[str]): List of test queries\n",
    "        reference_answers (List[str], optional): Reference answers for queries\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    results = []  # Initialize an empty list to store results\n",
    "    \n",
    "    # Iterate over each query in the test queries\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"Query: {query}\")  # Print the current query\n",
    "        \n",
    "        # Get reference answer if available\n",
    "        reference = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]  # Retrieve the reference answer for the current query\n",
    "        \n",
    "        # Compare hierarchical and standard RAG approaches\n",
    "        result = compare_approaches(query, pdf_path, reference)\n",
    "        results.append(result)  # Append the result to the results list\n",
    "    \n",
    "    # Generate overall analysis of the evaluation results\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,  # Return the individual results\n",
    "        \"overall_analysis\": overall_analysis  # Return the overall analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    Generate an overall analysis of the evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Results from individual query evaluations\n",
    "        \n",
    "    Returns:\n",
    "        str: Overall analysis\n",
    "    \"\"\"\n",
    "    # Define the system prompt to instruct the model on how to evaluate the results\n",
    "    system_prompt = \"\"\"You are an expert at evaluating information retrieval systems.\n",
    "Based on multiple test queries, provide an overall analysis comparing hierarchical RAG \n",
    "with standard RAG.\n",
    "\n",
    "Focus on:\n",
    "1. When hierarchical retrieval performs better and why\n",
    "2. When standard retrieval performs better and why\n",
    "3. The overall strengths and weaknesses of each approach\n",
    "4. Recommendations for when to use each approach\"\"\"\n",
    "\n",
    "    # Create a summary of the evaluations\n",
    "    evaluations_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
    "        evaluations_summary += f\"Hierarchical chunks: {result['hierarchical_chunks_count']}, Standard chunks: {result['standard_chunks_count']}\\n\"\n",
    "        evaluations_summary += f\"Comparison summary: {result['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "    # Define the user prompt with the evaluations summary\n",
    "    user_prompt = f\"\"\"Based on the following evaluations comparing hierarchical vs standard RAG across {len(results)} queries, \n",
    "provide an overall analysis of these two approaches:\n",
    "\n",
    "{evaluations_summary}\n",
    "\n",
    "Please provide a comprehensive analysis of the relative strengths and weaknesses of hierarchical RAG \n",
    "compared to standard RAG, with specific focus on retrieval quality and response generation.\"\"\"\n",
    "\n",
    "    # Make a request to the OpenAI API to generate the overall analysis\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"google/gemma-2-2b-it\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # User message with the evaluations summary\n",
    "        ],\n",
    "        temperature=0  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated overall analysis\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Hierarchical and Standard RAG Approaches\n",
    "## 分层RAG方法和标准RAG方法的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document and creating vector stores...\n",
      "Extracting text from data/AI_Information.pdf...\n",
      "Extracted 15 pages with content\n",
      "Generating page summaries...\n",
      "Summarizing page 1/15...\n",
      "Summarizing page 2/15...\n",
      "Summarizing page 3/15...\n",
      "Summarizing page 4/15...\n",
      "Summarizing page 5/15...\n",
      "Summarizing page 6/15...\n",
      "Summarizing page 7/15...\n",
      "Summarizing page 8/15...\n",
      "Summarizing page 9/15...\n",
      "Summarizing page 10/15...\n",
      "Summarizing page 11/15...\n",
      "Summarizing page 12/15...\n",
      "Summarizing page 13/15...\n",
      "Summarizing page 14/15...\n",
      "Summarizing page 15/15...\n",
      "Created 47 detailed chunks\n",
      "Creating embeddings for summaries...\n",
      "Creating embeddings for detailed chunks...\n",
      "Created vector stores with 15 summaries and 47 chunks\n",
      "Performing hierarchical retrieval for query: What are the key applications of transformer models in natural language processing?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alan_zp\\AppData\\Local\\Temp\\ipykernel_31140\\2918097221.py:62: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  \"similarity\": float(score)  # Add the similarity score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 relevant summaries\n",
      "Retrieved 10 detailed chunks from relevant pages\n",
      "\n",
      "=== Response ===\n",
      "The provided text doesn't explicitly detail the applications of transformer models in natural language processing. \n",
      "\n",
      "However, it does mention that **transformer models are used in various NLP tasks**, including:\n",
      "\n",
      "* **Language translation:**  Transformers are used to translate languages accurately and efficiently.\n",
      "* **Text summarization:**  They can analyze and condense large amounts of text into concise summaries.\n",
      "* **Sentiment analysis:**  Transformers can understand the emotional tone of text, identifying positive, negative, or neutral sentiment. \n",
      "\n",
      "To learn more about specific applications of transformer models in NLP, you can research further on the topic. \n",
      "\n",
      "Query: How do transformers handle sequential data compared to RNNs?\n",
      "\n",
      "=== Comparing RAG approaches for query: How do transformers handle sequential data compared to RNNs? ===\n",
      "\n",
      "Running hierarchical RAG...\n",
      "Loading existing vector stores...\n",
      "Performing hierarchical retrieval for query: How do transformers handle sequential data compared to RNNs?\n",
      "Retrieved 3 relevant summaries\n",
      "Retrieved 10 detailed chunks from relevant pages\n",
      "\n",
      "Running standard RAG...\n",
      "Extracting text from data/AI_Information.pdf...\n",
      "Extracted 15 pages with content\n",
      "Created 47 chunks for standard RAG\n",
      "Creating embeddings for chunks...\n",
      "Retrieved 15 chunks with standard RAG\n",
      "\n",
      "=== OVERALL ANALYSIS ===\n",
      "Let's dive into a comprehensive analysis of Hierarchical RAG vs. Standard RAG, focusing on their strengths and weaknesses in the context of information retrieval and response generation.\n",
      "\n",
      "**Hierarchical RAG (Hierarchical Recursive Attention-based Graph) **\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "* **Improved Information Retrieval:** Hierarchical RAG excels at retrieving information from a large knowledge base by leveraging a hierarchical structure. This structure allows it to:\n",
      "    * **Capture complex relationships:**  It can identify and connect concepts across different levels of abstraction, making it better at understanding the nuances of complex queries.\n",
      "    * **Handle long-range dependencies:**  Hierarchical structures can effectively capture relationships between distant concepts, which is crucial for understanding queries that involve multiple steps or require reasoning.\n",
      "    * **Reduce redundancy:**  By grouping related information into hierarchical chunks, it can avoid redundant information retrieval, leading to more concise and relevant responses.\n",
      "* **Enhanced Response Generation:**  The hierarchical structure allows for more sophisticated response generation by:\n",
      "    * **Providing context:**  Hierarchical chunks can be used to provide context for the response, making it more informative and relevant.\n",
      "    * **Generating multi-level responses:**  It can generate responses that are organized hierarchically, making it easier for users to understand the information.\n",
      "    * **Facilitating reasoning:**  The hierarchical structure can be used to guide the reasoning process, leading to more accurate and logical responses.\n",
      "\n",
      "**Weaknesses:**\n",
      "\n",
      "* **Computational Complexity:**  Hierarchical RAG can be computationally expensive, especially for large knowledge bases and complex queries. The process of building the hierarchical structure and retrieving information from it can be time-consuming.\n",
      "* **Limited Flexibility:**  The hierarchical structure can limit the flexibility of the system, making it less adaptable to queries that require unconventional or unexpected connections.\n",
      "* **Potential for Overfitting:**  The hierarchical structure can lead to overfitting if the training data is not diverse enough.\n",
      "\n",
      "\n",
      "**Standard RAG (Recursive Attention-based Graph)**\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "* **Simplicity and Efficiency:**  Standard RAG is generally simpler to implement and computationally less demanding than hierarchical RAG. This makes it more suitable for applications with limited computational resources.\n",
      "* **Flexibility:**  Standard RAG offers more flexibility in handling complex queries and relationships between concepts. It can be more adaptable to queries that require unconventional connections or unexpected relationships.\n",
      "* **Scalability:**  Standard RAG can be scaled more easily to handle large knowledge bases and complex queries.\n",
      "\n",
      "**Weaknesses:**\n",
      "\n",
      "* **Limited Information Retrieval:**  Standard RAG can struggle with retrieving information from large knowledge bases and complex queries. It may not be able to capture the nuances of complex relationships or long-range dependencies.\n",
      "* **Response Generation Challenges:**  Standard RAG can struggle to generate multi-level responses or provide context for the response. This can lead to less informative and less engaging responses.\n",
      "\n",
      "\n",
      "**Overall Analysis:**\n",
      "\n",
      "* **Retrieval Quality:**  Hierarchical RAG generally outperforms standard RAG in retrieving information from large knowledge bases and complex queries. It excels at capturing long-range dependencies and understanding the nuances of complex relationships.\n",
      "* **Response Generation:**  Hierarchical RAG can generate more sophisticated and contextually rich responses, making it more suitable for complex and nuanced queries.\n",
      "* **Computational Complexity:**  Standard RAG is generally more computationally efficient than hierarchical RAG.\n",
      "* **Flexibility:**  Standard RAG offers more flexibility in handling complex queries and relationships.\n",
      "\n",
      "**Recommendations:**\n",
      "\n",
      "* **Choose Hierarchical RAG:**  When dealing with complex queries, large knowledge bases, and the need for nuanced and contextually rich responses.\n",
      "* **Choose Standard RAG:**  When computational resources are limited, flexibility is crucial, and the query is relatively straightforward.\n",
      "\n",
      "**Key Considerations:**\n",
      "\n",
      "* **Query Complexity:**  The complexity of the query is a major factor in determining which approach is more appropriate.\n",
      "* **Knowledge Base Size:**  The size of the knowledge base also plays a role. Hierarchical RAG is more effective for large knowledge bases.\n",
      "* **Computational Resources:**  The available computational resources should be considered when choosing between the two approaches.\n",
      "\n",
      "\n",
      "Ultimately, the best approach depends on the specific needs of the application.  A thorough understanding of the query, the knowledge base, and the available resources is essential for making an informed decision. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to the PDF document containing AI information\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# Example query about AI for testing the hierarchical RAG approach\n",
    "query = \"What are the key applications of transformer models in natural language processing?\"\n",
    "result = hierarchical_rag(query, pdf_path)\n",
    "\n",
    "print(\"\\n=== Response ===\")\n",
    "print(result[\"response\"])\n",
    "\n",
    "# Test query for formal evaluation (using only one query as requested)\n",
    "test_queries = [\n",
    "    \"How do transformers handle sequential data compared to RNNs?\"\n",
    "]\n",
    "\n",
    "# Reference answer for the test query to enable comparison\n",
    "reference_answers = [\n",
    "    \"Transformers handle sequential data differently from RNNs by using self-attention mechanisms instead of recurrent connections. This allows transformers to process all tokens in parallel rather than sequentially, capturing long-range dependencies more efficiently and enabling better parallelization during training. Unlike RNNs, transformers don't suffer from vanishing gradient problems with long sequences.\"\n",
    "]\n",
    "\n",
    "# Run the evaluation comparing hierarchical and standard RAG approaches\n",
    "evaluation_results = run_evaluation(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "# Print the overall analysis of the comparison\n",
    "print(\"\\n=== OVERALL ANALYSIS ===\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

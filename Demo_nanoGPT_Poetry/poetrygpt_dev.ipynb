{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "## Building a GPT\n",
    "\n",
    "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "O6medjfRsLD9"
   },
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('combined_poetry.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characterss:  23376050\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c5V0FvqseE0",
    "outputId": "25ca7adc-b8c0-42d1-b08c-e0863c5c314e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "半世为人，不曾教大人心困。\n",
      "虽是搽胭粉，只争不裹头巾，将那等不做人的婆娘恨。\n",
      "男儿人若不依本分，一个抢白是非两家分。\n",
      "壮鼻凹硬如石铁，教满耳根都做了烧云。\n",
      "普天下汉子尽教都先有意，牢把定自己休不成人。\n",
      "虽然两家无意，便待一面成亲，不分晓便似包着一肚皮干牛粪。\n",
      "知人无意，及早抽身。\n",
      "等不得水温，一声要面盆；恰递与面盆，一声要手巾；却执与手巾，一声解纽门。\n",
      "使的人无淹润、百般支分！。\n",
      "入得房门，怎回身？厅独卧房儿窄窄别别，有甚铺呈？燕燕己身有甚么孝顺？拗不过哥哥行在意殷勤。\n",
      "卧地观经史，坐地对圣人。\n",
      "你观的国风、雅、颂施诂训，诵的典谟训诰居尧舜，（末云了）（正旦唱）说的温良恭俭行忠信。\n",
      "燕燕子理会得龙盘虎踞灭燕齐，谁会甚儿婚女聘成秦晋？（末云）这书院好。\n",
      "这书房存得阿马，会得客宾。\n",
      "翠筠月朗龙蛇印，碧轩夜冷灯香信，绿窗雨细琴书润。\n",
      "每朝席上宴佳宾，抵多少\"十年窗下无人问\"！（末云住）（正旦唱）。\n",
      "更做道一家生女，百家求问。\n",
      "才说贞烈，那里取一个时辰？见他语言儿栽排得淹润，怕不待言词硬，性格村、他怎比寻常世人？（末云了）（正旦唱）。\n",
      "无男儿只一身，担寂寞受孤闷；有男儿呓梦入劳魂，心肠百处分。\n",
      "知得有情人不曾来问肯，便待要成眷姻。\n",
      "自勘婚，自说亲，也是\"贱媳妇责媒人\"。\n",
      "往常我冰清玉洁难亲近，是他亲，子管教话儿亲。\n",
      "我煞待嗔，我便恶相闻。\n",
      "怕不依随蒙君一夜恩，争奈忒达地、忒知根，兼上亲上成亲好对门。\n",
      "觑了他兀的模样，这般身分。\n",
      "若脱过这好郎君。\n",
      "教人道\"眼里无珍一世贫\"；成就了又怕辜恩。\n",
      "若往常烈焰飞腾情性紧，若一遭儿恩爱，再来不问，枉侵了这百年恩。\n",
      "一个个背槽抛粪，一个个负义忘恩，自来鱼雁无音信。\n",
      "自思忖，不审得话儿真，枉葫芦提了\"燕尔新婚。\n",
      "忽地却掀帘，兜地回头问，不由我心儿里便亲。\n",
      "你把那并枕睡的日头儿再定轮，休教我逐宵价握雨携云。\n",
      "过今春，先教我不系腰裙，便是半簸箕头钱扑个复纯。\n",
      "教人道\"眼里有珍\"，你可休\"言而无信\"！（云）许下我包髻、团衫、绣手巾。\n",
      "专等你世袭千户的小夫人。\n",
      "（下）。\n",
      "年例寒食，邻姬每斗来邀会，去年时没人将我拘管收拾。\n",
      "打秋千，闲斗草，直到个昏天黑地；今年个不敢来迟，有一个未拿着性儿女婿。\n",
      "因甚把玉粳米牙儿抵，金莲花攒枕倚？或嗔或喜脸儿多？哎！你、你！教我没想没思，两心两意，早晨古自一家一计！。\n",
      "莫不是郊外去逢着甚邪祟？又不疯又不呆痴，面没罗、呆\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e-Rbyr8sfM8",
    "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"()*+,-/0123456789:;<=>?ACDEGHLMNRSTV[]abcdefghilmnopqrstuvxy{|} ·òōΑΘΙΛχЁКЛМНбвись‘’“”…ⅲⅶ②┒┭┾╆□○●⻊⿰⿱　、。〈〉《》「」『』〔〕〖〗ぇえくぐけさざそちぢづどぴまみめやゐァコシセソゾタドネビブホリンヴヵㄇㄉㄋㄎㄏㄐㄓㄔㄛㄝㄡㄤ㐷㑃㑛㑩㑭㑹㑻㒔㒟㒦㒩㒿㓠㓢㓤㔃㔅㔇㔉㔢㔩㕒㕙㕭㕮㖀㖃㖇㖞㖠㖶㖷㖿㗀㗌㗘㗚㗦㗫㗭㗱㗶㗻㘆㘈㘋㘖㘞㘭㘱㘲㙞㙠㙲㙺㙻㚇㛂㛐㛠㛪㛹㜕㜪㜮㝅㝠㝡㝢㝵㟅㟍㟏㟝㟞㟢㟥㟧㟮㟯㟳㟼㠁㠂㠌㠔㠗㠢㠥㠭㠹㠾㡊㡚㡛㡜㡠㢩㢮㣂㣚㣻㤌㤝㤞㤤㤯㥀㥄㥏㥟㥪㥾㦨㦬㦸㧐㧑㧗㧙㧝㧞㧹㧻㨀㨋㨏㨖㨝㨟㨫㨭㨳㩇㩈㩉㩦㩧㩻㪇㪍㫋㫙㫝㫰㬈㬉㬊㬋㬒㬠㬢㭊㭏㭫㭬㭲㭿㮇㮕㮬㮰㯕㰂㰅㰕㰞㰤㰦㰹㱔㱥㲇㲉㲏㲚㲞㲪㲯㲲㳂㳇㳠㳷㴩㴲㵝㵳㵿㶁㶄㶉㶏㶒㶼㶿㷎㷟㸃㸌㸐㸑㸔㸙㹀㹞㹠㹱㹶㺃㺍㺑㺗㺠㺥㺦㺳㺷㻏㻞㼌㼐㼘㼚㽅㽘㽬㾌㾓㾕㾪㿉㿋㿪䀡䀢䀣䀨䀪䀭䀽䁈䁔䁠䁭䁸䁹䁾䂍䂓䃁䃂䃅䃉䃔䃜䃣䃥䃧䃬䃭䃴䃸䃺䄠䄡䄩䅉䅎䅗䅟䅤䆕䆗䆛䆷䆿䇓䇲䇹䈂䈉䈕䈝䈴䈻䉂䉉䉛䉡䉤䉦䊗䊶䋏䌨䌴䌷䌸䌹䍀䍁䍐䍙䍠䍡䍥䍧䍴䎂䎖䎘䎧䎫䎲䎳䏝䏰䏶䏺䐊䐑䐹䐿䑔䑦䑧䑰䑲䑳䑿䒠䒶䒼䓑䓕䓖䓗䓞䓨䓪䓬䔄䔧䔩䔫䔲䔾䔿䕓䕠䕡䕢䕩䕭䕶䕷䕸䕹䖃䖏䖘䖜䖟䖦䖴䗁䗐䗖䗻䘓䘢䘧䘨䘿䙀䙰䚢䚥䚦䚧䛏䛠䛩䜕䜝䜣䜩䜫䜺䝙䝟䝤䞘䟂䟫䠊䠞䠥䡌䡝䡬䡴䡾䢆䢇䢼䣔䣢䣱䤑䤖䤙䤴䥓䥶䥽䦨䦪䦱䧆䧟䨏䨚䨟䨥䨧䨲䨴䩉䩋䩡䩨䩫䩬䩮䪅䪌䪓䪜䪥䪫䪻䫇䫉䫌䫒䫜䫥䫫䫲䫹䫿䬂䬌䬐䬦䬪䬴䬵䮂䮧䯄䯅䯔䯗䯰䯱䯲䯳䰀䰃䰄䰉䰎䰐䰒䰔䰠䱐䱜䱠䱥䱹䲔䲡䲺䳏䳒䳟䳰䳷䳺䴊䴏䴔䴕䴖䴙䴠䴥䴮䴵䴺䵃䵎䵬䵷䶉䶎䶥䶪䶮一丁七万丈三上下不与丏丐丑专且丕世丗丘丙业丛东丝丞両丢两严丧丨个丫中丰丱串丳临丶丷丸丹为主丽举丿乂乃久么义之乌乍乎乏乐乒乓乔乖乗乘乙乚乜九乞也习乡乣书乩买乱乳乹乾亀亁了予争亊事二亍于亏云互五井亘亚些亟亠亡亢交亥亦产亨亩享京亭亮亰亲亳亵亶亷亸亹人亻亾亿什仁仂仃仄仅仆仇仉今介仍从仑仓仔仕他仗付仙仚仝仞仟仡代令以仪们仭仰仲仳仵件价任份仾仿企伃伇伈伉伊伋伍伎伏伐休伕众优伙会伛伞伟传伡伣伤伥伦伧伪伫伭伯估伴伶伸伹伺伻似伽伾佀佁佃佅但佉佋佌位低住佐佑佒体何佖佗佘余佚佛作佝佞佟你佣佥佨佩佪佯佰佲佳佶佷佹佺佻佼佽佾使侁侂侃侄侅侈侉例侍侏侐侑侔侗侘供侜依侠侣侥侦侧侨侩侪侫侬侮侯侰侲侵侹侻便俀促俄俅俇俉俊俋俎俏俐俑俗俘俙俚俛俜保俞俟信俣俦俨俩俪俫俭修俯俱俳俵俶俸俺俾倂倅倍倏倐倒倓倔倕倘候倚倛倜借倠倡倢倣倥倦倧倨倩倪倬倭倮倰倴倶债倻值倾偁偃偄偅假偈偊偋偌偎偏偓偕偘做停偠偡偢健偪偫偬偭偲偶偷偹偻偾偿傀傁傃傅傈傋傍傒傔傕傚傜傝傞傥傦傧储傩傪傫催傱傲傺傻僁僄僇僊働僎像僒僔僖僚僛僜僝僣僦僧僩僪僬僭僮僯僰僳僴僵僶僸僻僽僾僿儃儆儇儋儌儑儒儓儗儚儛儜儞儡儤儦儧儱儳儴儵儹儽儿兀允元兄充兆先光克免兎兏児兑兔兕兖党兜兟兠兢入全兪八公六兮兰共关兴兵其具典兹养兼兽冀冁冃内冇冈冉冋册再冏冐冒冓冔冕冖冗冘写军农冞冠冡冢冤冥冦冨冩冫冬冯冰冱冲决况冶冷冸冺冻冼冽净凂凃凄准凇凉凊凋凌凎减凑凓凖凗凘凚凛凝凟几凡凢凤凫凭凮凯凰凳凴凶凷凸凹出击函凾凿刀刁刂刃刄刅分切刈刊刋刌刍刎刑划刓刔刖列刘则刚创刜初删判刦刧刨利别刬刭刮到刱刲刳刵制刷券刹刺刻刼刽刾刿剀剁剂剃剉削剌前剏剐剑剔剕剖剙剚剜剞剟剠剡剣剥剧剨剩剪剭副剰割剳剶剸剺剽剿劀劁劂劄劈劋劎劒劓劔劖劘劙力劝办功加务劢劣动助努劫劬劭励劲劳劵効劻劼劾势勃勅勇勉勋勌勍勒勔勖勗勘勚募勠勤勦勮勰勲勷勺勾勿匀匃匄包匆匈匉匊匋匌匍匏匐匔匕化北匙匜匝匠匡匣匦匪匮匰匲匳匴匵匶匹区医匼匽匾匿十卂千卄卅升午卉半卍华协卐卑卒卓单卖南博卜卝卞卟占卢卣卤卦卧卨卫卬卭卮卯印危卲即却卵卷卸卺卼卿厂厄厅历厇厈厉压厌厍厎厐厓厔厕厖厘厚厜厝厞原厢厣厥厦厨厩厪厫厮厯厰厶去县叁参叅叆叇又叉及友双反収发叔取受变叙叚叛叟叠叡口古句另叨叩只叫召叮可台叱史右叵叶号司叹叻叽叿吁吃各吆合吉吊同名后吏吐向吓吕吖吗吚君吝吞吟吠吡否吧吨吩吪含听吭吮启吰吴吵吷吸吹吺吻吼吽吾吿呀呃呅呆呇呈呉告呋呌呐呑呓呕呖呗员呙呛呜呝呞呢呤呥呦周呪呫呬呰呱呲味呴呵呶呷呸呺呻呼命呾呿咀咂咄咆咈咋和咍咎咏咐咒咔咕咙咚咛咜咡咢咤咥咦咨咩咫咬咭咮咯咱咲咳咷咸咺咻咽咿哀品哂哄哆哇哈哉哌响哎哏哑哒哓哔哕哗哙哜哝哟哢哤哥哦哨哩哪哭哮哰哲哳哶哺哻哽哾哿唁唅唆唇唈唉唎唏唐唑唔唘唝唠唣唤唦唧唪唫唬售唯唱唲唳唵唶唼唽唾唿啀啁啄啅商啇啉啊啍啐啑啒啕啖啗啙啜啝啡啣啧啨啬啭啮啰啴啸啻啼啽啾喁喂喃善喆喇喈喉喊喋喌喍喏喑喓喔喘喙喛喜喝喟喢喣喤喦喧喨喩喭喰喱喳喷喻喽喾喿嗁嗂嗃嗄嗅嗈嗉嗋嗌嗍嗏嗑嗒嗓嗔嗗嗛嗜嗝嗟嗢嗣嗤嗥嗨嗫嗲嗳嗷嗸嗺嗽嗾嗿嘀嘂嘅嘇嘈嘉嘌嘎嘏嘐嘑嘒嘕嘘嘛嘤嘨嘬嘱嘲嘴嘶嘷嘹嘻嘼嘿噀噂噆噇噉噌噍噎噏噐噑噒噙噜噡噢噣噤器噩噪噫噬噭噰噱噷噽嚂嚃嚄嚅嚆嚊嚎嚏嚓嚗嚘嚚嚢嚣嚤嚫嚬嚭嚱嚵嚷嚻嚼嚾囄囇囊囋囏囐囓囗囘囙囚四囝回因团囤囦囨囫囬园囮困囱围囵囷囹固国图囿圀圁圂圃圄圆圈圉圊圌圎圏圑圗圚圛圜圝土圠圢圣在圩圪圬圭圮圯地圳圴圹场圻圼圾圿址坁坂坃坆均坊坋坌坍坎坏坐坑块坚坛坜坝坞坟坠坡坤坦坨坩坪坫坯坰坱坲坳坵坷坻坼垂垄垅垆型垌垍垒垓垔垕垖垗垛垜垝垞垠垡垢垣垤垦垩垫垭垲垸垾埀埂埃埆埇埊埋埌城埏埒埓埔埕埘埙埚埛埜埞域埠埤埦埧埭埯埲埳埴埵埶埸培基埻埼埽埿堀堁堂堄堆堇堈堋堌堍堑堕堘堙堛堞堠堡堤堦堧堨堪堭堮堰堲堵堶堽塆塈塉塌塍塑塓塔塕塘塜塞塠塩填塯塲塳塺塼塽塾墀墁境墄墅墉墋墌墍墐墓墖増墙墝增墟墢墥墦墨墩墪墯墺墼壀壁壃壅壊壌壍壑壒壕壖壝壤壥士壬壮声壳壴壵壶壷壸壹壻处夆备夊夋夌复夏夐夑夔夕外夗夘夙多夜够夣夤大天太夫夬夭央夯失夲头夷夸夹夺奁奂奄奅奇奈奉奊奋奎奏契奓奔奕奖套奘奚奠奡奢奣奥奨奫奭奯奰女奴奶奸她好妁如妃妄妆妇妈妉妊妍妒妓妖妙妞妠妣妤妥妨妩妪妫妬妮妯妲妳妵妷妸妹妺妻妾姀姁姆姈姉姊始姌姐姑姒姓委姗姘姙姚姜姝姞姡姢姣姤姥姧姨姪姫姬姮姱姳姸姹姻姿娀威娃娄娅娆娇娈娉娌娑娒娓娖娗娘娜娞娟娠娣娥娩娬娭娯娱娲娴娵娶娼娿婀婆婉婐婑婓婕婗婘婙婚婞婠婢婣婥婪婬婳婴婵婶婷婺婿媂媆媌媍媒媓媕媖媙媚媛媞媟媠媢媥媪媭媮媱媲媳媵媷媸媺媻媾媿嫁嫂嫄嫇嫈嫉嫌嫓嫔嫕嫖嫚嫛嫜嫠嫡嫣嫦嫧嫩嫪嫫嫭嫮嫰嫱嫲嫶嫷嬉嬖嬗嬛嬝嬭嬲嬴嬷嬺嬾嬿孀孅孏子孑孔孕字存孙孚孛孜孝孟季孤孥学孩孪孰孱孳孵孺孽宀宁宂它宄宅宇守安宋完宍宎宏宓宕宗官宙定宛宜宝实宠审客宣室宥宦宧宪宫宭宰害宴宵家宷宸容宻宼宽宾宿寁寂寃寄寅密寇寋富寎寐寑寒寓寔寕寖寗寘寛寜寝寞察寡寤寥寨寮寰寱寳寷寸对寺寻导寽寿尀封専尃射尅将尉尊小少尔尖尘尙尚尝尟尠尢尤尧尨尩尪尫尬尭尯尰就尴尵尸尹尺尻尼尽尾尿局屁层屃居屈屉届屋屎屏屐屑展屙屝属屠屡屣履屦屧屫屭屮屯山屴屹屺屼屿岀岁岂岅岆岇岈岉岊岋岌岍岏岐岑岓岔岖岗岘岚岛岜岝岞岢岣岥岧岨岩岪岫岬岭岮岯岰岱岳岴岵岷岸岹岿峁峃峄峉峋峍峎峐峒峓峗峙峛峝峞峡峣峤峥峦峧峨峩峪峬峭峮峰峵峺峻峿崃崄崅崆崇崈崊崎崏崐崒崔崕崖崘崚崛崝崟崤崦崧崨崩崪崫崭崯崱崲崳崴崷崺崽崾崿嵁嵂嵃嵇嵊嵋嵌嵍嵎嵑嵒嵓嵔嵕嵘嵚嵛嵜嵝嵞嵠嵡嵥嵦嵨嵩嵪嵫嵬嵯嵰嵱嵲嵳嵵嵷嵸嵺嶂嶃嶅嶆嶉嶊嶐嶒嶓嶕嶙嶚嶛嶝嶞嶣嶤嶪嶫嶭嶰嶱嶲嶴嶶嶷嶻嶾巀巂巃巄巅巇巉巌巍巏巑巓巙川州巡巢工左巧巨巩巫差己已巳巴巵巷巻巽巾巿帀币市布帅帆师帉帊帋希帏帐帑帒帓帔帕帖帗帘帙帚帛帜帝帟帡帢帣帤带帧帨帩帬席帮帯帱帲帷常帻帼帽幂幄幅幈幌幍幏幐幔幕幖幙幛幞幡幢幧幨幪幭幮幰幱干平年幵并幷幸幺幻幼幽广庀庁庄庆庇庉床庋庌序庐庑库应底庖店庘庙庚府庞废庠庤庥度座庨庪庬庭庮庱庳庵庶康庸庻庼庾廅廇廉廊廋廌廏廐廑廒廓廔廖廗廙廛廜廤廥廨廪廰廱廵延廷廹建廻廼廽廾廿开弁异弃弄弅弆弇弈弊弋式弑弓引弗弘弛弜弝弟张弢弥弦弧弨弩弭弮弯弰弱弲弸弹强弼彀彊彍彐归当录彖彗彘彚彛彜彝彞彟彡形彣彤彦彧彨彩彪彬彭彯彰影彳彴彷役彻彼彾往征徂徃径待徇很徉徊律徍徐徒従徕得徘徙徜御徤徧徨循徬徭微徯徳徴徵德徼徽忀心忄必忆忉忌忍忏忐忑忒忓忔忖志忘忙忝忠忡忤忧忩忪快忭忮忱忲忳念忷忸忺忻忼忽忾忿怀态怂怃怅怆怊怍怎怏怐怒怓怔怕怖怗怙怛怜思怞怠怡怢急怦性怨怩怪怫怮怯怱怲怳怵总怼怿恀恁恂恃恄恅恇恈恉恊恋恌恍恐恒恓恔恕恙恚恛恝恞恟恠恡恢恣恤恧恨恩恪恫恬恭息恰恲恳恵恶恸恹恺恻恼恽恿悁悃悄悇悉悊悋悌悍悒悔悕悖悗悚悛悝悟悠悢患悤悦您悫悬悭悮悯悰悱悲悴悷悸悹悺悻悼悾惄情惆惇惉惊惋惎惏惑惓惔惕惘惙惚惛惜惝惞惟惠惣惥惧惨惩惫惬惭惮惯惰想惴惵惶惷惸惹惺惽愀愁愆愈愉愊愍愎意愐愒愓愔愕愗愙愚愞感愠愡愤愦愧愫愬愯愰愵愶愸愺愽愿慁慂慅慆慇慈慉慊慌慎慑慓慕慙慜慝慞慠慢慥慦慧慨慬慭慰慱慴慵慷慸慻慽慿憀憁憃憇憍憎憓憔憕憗憘憙憛憝憞憣憧憨憩憪憬憭憯憺憾懂懃懅懆懈懊懋懐懑懒懔懕懘懜懡懦懫懭懮懰懱懴懵懹懽懿戁戃戄戅戆戈戊戋戌戍戎戏成我戒戕或戗战戙戚戛戞戟戡戢戣截戫戬戭戮戯戳戴戵户戸戹戺戽戾房所扁扂扃扄扅扆扇扈扉扊手扌才扎扐扑扒打扔托扛扞扟扠扡扢扣扤扦执扩扪扫扬扭扮扯扰扱扳扴扵扶扷批扺扼找承技抃抄抅抆抉把抋抏抑抒抓抔投抖抗折抚抛抜抝抟抠抡抢护报抨披抬抰抱抳抵抶抷抸抹抺押抽拂拄担拆拇拈拉拊拌拍拎拏拐拑拒拓拔拕拖拗拘拙拚招拜拟拢拣拥拦拧拨择拪拫括拭拮拯拱拲拳拴拶拷拺拼拽拾拿持挂挃挅指挈按挍挎挏挐挑挒挖挚挛挜挝挞挟挠挡挢挣挤挥挦挨挪挫振挲挳挶挹挺挻挼挽挿捂捃捄捆捈捉捋捌捍捎捏捐捓捕捖捘捜捝捞损捡换捣捥捧捩捬捭据捶捷捺捻捽捾掀掁掂掇授掉掊掌掎掏掐排掔掖掘掝掞掠探掣掤接掦控推掩措掫掬掭掮掯掰掲掳掴掷掸掺掻掼掾掿揁揃揄揆揈揉揌揍揎描提揑插揔揕揖揗揜揞揠握揣揥揩揪揫揬揭揲揳援揵揶揷揸揺揽揾揿搀搁搂搃搅搆搉搊搋搏搐搒搓搔搕搘搜搟搠搢搤搥搦搧搨搪搬搭搯搰搳搴携搽摃摄摅摆摇摈摉摊摎摏摒摔摖摘摚摛摝摠摧摩摫摭摮摰摲摴摵摸摹摽撁撃撄撅撆撇撋撑撒撕撖撗撘撙撚撜撞撡撢撤撦撩播撮撰撱撶撷撸撹撺撼撽擂擅擈擉操擎擐擑擒擕擖擗擘擞擡擢擤擥擦擧擨擩擪擭擸擿攀攂攃攅攉攎攑攒攓攕攘攞攟攡攥攦攧攫攮支攲攴攵收攷攸改攻攽放政敂故敇效敉敌敍敏救敔敕敖教敚敛敜敝敞敢散敦敧敩敬敭数敲敳整敶敷敺敻斁文斈斋斌斎斐斑斒斓斔斖斗料斚斛斜斝斟斡斤斥斧斨斩斫断斮斯新斲斵斶方於施斾斿旁旃旄旅旆旇旈旉旋旌旍旎族旐旒旓旔旖旗旙旛旜旝旞旟无旡既日旦旧旨早旬旭旰旱旲旳旴旵时旷旸旹旺旻旼昀昂昃昄昆昇昈昉昊昋昌明昏昐昒易昔昕昙昜昝昞星映昡春昧昨昩昪昫昬昭是昱昲昳昴昵昶昷昺昻昼昽显晀晁晃晅晋晌晏晒晓晔晕晖晙晚晜晞晟晡晢晣晤晥晦晧晨晩晬普景晰晱晲晳晴晶晷晹智晻晼晾暀暁暂暄暇暋暌暍暎暏暑暖暗暝暠暡暥暧暨暪暮暱暳暴暵暸暹暾曀曈曌曒曙曚曛曜曝曡曣曦曩曭曰曲曳更曵曶曷曹曺曼曽曾替最朂朄朅月有朊朋朌服朎朏朐朒朓朔朕朗朘朙望朝朞期朡朢朣朦木未末本札术朱朴朵朶机朽朾朿杀杂权杄杅杆杇杈杉杌李杏材村杓杕杖杗杙杜杝杞束杠条来杨杪杫杭杮杯杰杲杳杴杵杶杷杸杻杼松板极枃构枅枇枉枊枋枌枍枏析枑枒枓枕林枘枚果枝枞枡枢枣枤枥枦枧枨枪枫枬枭枮枯枰枲枳枵架枷枸枹枻枿柀柁柂柄柅柆柈柍柎柏某柑柒染柔柖柘柙柚柜柝柞柟柠柢柣柤查柦柧柩柪柫柬柮柯柰柱柲柳柴柷柸柹柽柿栀栅标栈栉栊栋栌栎栏树栒栓栔栖栗栘栛栝栞栟校栢栥栧栩株栭栮栰栱栲栳栴样核根栺栻格栽栾桀桁桂桃桄桅框案桉桋桌桍桎桐桑桒桓桔桕桚桞桠桡桢档桤桥桦桧桨桩桫桭桮桯桲桴桶桷桹桺桻桼梀梁梃梅梆梌梏梐梓梗梙梠梡梢梦梧梨梩梫梬梭梮梯械梱梳梴梵梼检棁棂棃棆棉棋棌棍棎棐棒棓棕棘棙棚棠棣棦棨棫棬森棰棱棳棵棹棺棻棼棽棿椀椁椂椄椅椆椇椈椌植椎椐椑椒椓椔椗椘椟椠椤椫椭椮椰椳椴椵椶椷椸椹椼椽椾椿楀楁楂楅楋楔楖楗楘楙楚楛楝楞楟楠楡楢楣楥楦楩楪楫楬楮楯楰楱楳楶楷楸楹楼概榄榅榆榇榈榉榍榎榐榑榔榕榛榜榝榞榠榤榥榧榨榭榰榱榴榷榸榹榻榼榾槀槁槃槅槆槊槌槎槐槔槖様槚槛槜槟槠槢槥槦槩槬槭槱槲槴槵槷槸槹槽槿樀樊樏樐樔樕樗樚樛樝樟模樨横樬樯樱樲樴樵樷樻樽樾橄橅橆橇橊橎橐橑橘橙橚橛橜橞橡橤橦橧橰橱橹橺橼檀檃檄檇檈檎檏檐檑檗檛檝檞檠檥檨檬檾檿櫁櫂櫉櫋櫐櫑櫖櫜櫩櫰櫴櫹櫺櫽欀欂欃欈欎欔欗欙欛欝欠次欢欣欤欦欧欬欱欲欵欶欷欸欹欺欻款欿歃歅歆歇歈歉歊歋歌歏歔歕歖歗歘歙歛歜歝歠止正此步武歧歩歪歭歰歳歴歹死歼殀殁殂殃殄殆殇殉殊残殍殑殒殓殕殖殗殙殚殛殟殡殣殪殱殳殴段殷殽殿毁毂毅毈毉毋毌母毎每毐毒毓比毕毖毗毘毙毚毛毡毣毦毨毫毬毯毰毱毳毵毶毷毸毹毺毼毾氃氄氅氉氊氋氍氎氏氐民氓气氛氜氤氧氯氲水氵氶氷永汀汁求汃汇汉汊汍汏汐汑汒汔汕汗汚汛汜汝汞江池污汤汦汧汨汩汪汭汰汲汳汴汶汸汹汽汾沁沂沃沄沅沆沇沈沉沋沌沍沎沏沐沓沔沕沗沘沙沚沛沜沟沠没沣沤沥沦沧沨沩沪沫沬沭沮沱沲河沴沸油治沼沽沾沿泂泄泅泆泉泊泌泎泐泑泒泓泔法泖泗泙泚泛泜泞泠泡波泣泥注泪泫泬泮泯泰泱泲泳泷泸泹泺泻泼泽泾泿洁洄洆洊洋洌洎洏洐洑洒洓洔洗洘洙洚洛洞洟洢津洦洧洨洪洫洮洱洲洳洴洵洷洸洹洺活洼洽派洿流浃浄浅浆浇浈浉浊测浍济浏浐浑浒浓浔浕浘浙浚浛浜浞浟浡浢浣浤浥浦浩浪浮浯浰浴浶海浸浺浻浼浽涂涅消涉涊涌涍涎涏涑涒涓涔涕涖涘涙涚涛涝涞涟涠涡涢涣涤润涧涨涩涪涫涬涭涮涯液涳涴涵涶涷涸涹涾涿淀淂淄淅淆淇淈淊淋淌淍淎淏淑淔淖淗淘淙淛淜淝淞淟淡淢淤淦淫淬淮淯淰深淲淳混淸淹添淼渀清渇渊渌渍渎渐渑渒渔渕渗渚渜渝渟渠渡渣渤渥渧温渫渭港渰渱渲渴游渹渺渻渼湀湃湄湆湉湌湍湎湏湑湒湓湔湖湗湘湛湜湝湟湠湡湢湣湩湫湮湱湲湴湼湾湿溃溅溆溇溉溏源溓溔溘溜溞溟溠溢溥溧溪溯溰溱溲溴溵溶溷溺溻溽滀滁滂滃滆滇滈滉滋滍滏滑滓滔滕滚滛滞滟滠满滢滣滤滥滦滨滩滪滫滮滴滵滹漂漅漆漈漉漎漏漑漒漓演漕漘漙漝漠漡漥漦漩漪漫漭漯漰漱漳漴漶漺漻漼漾潀潄潆潇潈潊潋潍潎潏潒潓潗潘潜潞潟潠潡潢潦潨潩潫潬潭潮潴潵潸潺潻潼潾澁澂澄澈澉澌澍澎澑澒澓澘澛澜澟澡澣澥澧澨澳澴澶澷澹澼澽澿激濂濅濇濈濉濊濋濎濑濒濓濔濙濞濠濡濩濪濬濭濮濯濲濳濴濵濶瀁瀄瀊瀌瀍瀎瀑瀔瀖瀚瀛瀜瀡瀣瀥瀩瀫瀬瀯瀴瀵瀹瀺瀼瀿灀灂灇灈灉灊灋灌灎灏灔灜灞灢灨火灬灭灯灰灵灶灸灺灼灾灿炀炁炅炉炊炎炏炒炔炕炖炙炚炜炝炟炤炧炫炬炭炮炯炰炱炲炳炴炷炸点炼炽烁烂烈烊烋烓烔烕烖烘烙烚烛烜烝烟烦烧烨烬热烰烱烹烺烻烽焄焇焉焊焌焐焕焘焙焚焜焞焠焥焦焫焭焮焯焰焱然焼煁煅煆煊煌煎煏煑煕煖煗煜煞煠煤煦照煨煮煴煼煽煿熀熁熄熇熈熊熏熔熖熙熛熝熟熠熢熣熨熬熭熯熳熸熹熺熻熿燂燃燄燋燎燏燐燑燔燕燝燠燣燥燧燮燹燿爀爁爆爊爋爌爑爓爕爗爚爝爞爟爢爣爨爪爬爮爰爱爵父爷爹爻爼爽牁牂片版牋牌牍牎牏牐牑牒牓牕牖牗牙牛牜牝牟牡牢牣牦牧物牯牱牲牴牵牷牸特牺牼牾牿犀犁犂犄犇犉犊犍犒犗犘犠犦犨犩犬犭犯犴状犷犸犹犺犿狁狂狃狄狅狈狉狌狍狎狐狒狔狖狗狘狙狚狝狞狟狠狡狢狤狥狦狨狩狫独狭狮狯狰狱狲狴狵狶狷狸狺狻狼猃猇猊猋猎猒猓猕猖猗猘猛猜猝猢猥猧猨猩猪猫猬猭献猯猰猱猲猳猴猵猷猺猾猿獌獍獐獑獒獓獖獗獚獝獞獠獡獢獦獧獬獭獯獳獶獹獾獿玃玄玅玆率玈玉玊王玎玏玑玒玓玕玖玗玘玙玚玛玞玠玡玢玦玨玩玫玭玮环现玱玲玳玷玺玻玼玾玿珀珂珃珇珈珉珊珌珍珎珏珑珓珙珝珞珠珣珤珥珦珧珩珪珫班珮珰珵珷珸珽琁球琄琅理琇琉琊琎琏琐琕琖琚琛琡琢琤琥琦琧琨琪琫琬琭琮琯琰琲琳琴琵琶琹琼瑀瑁瑄瑇瑉瑊瑑瑕瑗瑘瑙瑚瑛瑜瑝瑞瑟瑠瑢瑦瑨瑫瑭瑮瑯瑰瑱瑳瑴瑶瑷瑸瑺瑾瑿璀璁璃璅璆璇璈璋璎璐璖璘璚璜璞璟璠璥璧璨璩璪璬璲璹璺瓀瓃瓈瓉瓌瓐瓒瓓瓖瓘瓜瓝瓞瓟瓠瓢瓣瓤瓦瓬瓮瓯瓴瓶瓷瓻瓽瓿甀甃甄甆甈甋甍甎甑甒甓甔甖甗甘甚甜甝甞生甡甤甥用甪甫甬甭田由甲申电甶男甸甹町画甽甾甿畀畅畆畇畈畊畋界畎畏畐畒畔畕留畚畛畜畞畟畡畤略畦畧畨番畬畮畯畱畲畴畷畸畹畺畽畿疁疃疄疆疈疉疋疎疏疐疑疒疓疔疕疖疗疚疟疠疡疢疣疤疥疪疫疬疮疯疰疱疲疳疴疵疷疹疻疼疽疾疿痀痁痂痃病痆症痈痉痊痌痍痎痏痑痒痔痕痖痗痘痛痝痞痟痡痢痣痤痦痨痩痪痫痯痰痱痴痹痺痼痿瘀瘁瘃瘅瘆瘇瘈瘉瘏瘐瘕瘖瘗瘘瘙瘝瘟瘠瘢瘣瘤瘥瘦瘨瘫瘯瘰瘳瘴瘵瘸瘼瘽瘾瘿癃癈癊癋癏癕癖癗癙癛癞癠癣癦癫癯癸登白百癿皂皃的皆皇皈皋皎皐皑皓皖皙皛皜皞皠皡皤皥皦皪皭皮皯皱皲皴皵皷皻皾皿盂盆盈益盌盍盎盏盐监盒盔盖盗盘盛盝盟盥盦盩盫盬盭目盯盰盱盲盳直相盹盻盼盾眀省眂眄眇眈眉眊看眍眎眑眒眓眕眘眙眚真眠眡眢眤眦眧眨眩眬眭眯眴眵眶眷眸眹眺眼眽着睁睂睃睄睅睆睇睋睎睐睑睒睖睗睘睚睛睟睠睡睢督睥睦睨睩睪睫睬睲睴睹睺睽睿瞀瞅瞆瞇瞉瞋瞌瞍瞎瞐瞑瞒瞖瞙瞠瞢瞥瞧瞩瞪瞬瞭瞯瞰瞳瞵瞷瞹瞻瞽瞿矂矄矊矌矍矐矒矕矖矗矙矛矜矝矞矟矢矣知矦矧矩矫矬短矮矰矱矲石矴矶矸矹矻矼矾矿砀码砂砄砅砆砉砌砍砎砐砑砒研砕砖砗砚砟砠砢砣砥砦砧砭砮砯砰砲砳破砸砺砻砾础硁硉硊硋硌硍硎硏硐硔硕硖硗硙硝硞硠硩硪硫硬硭确硱硼硾硿碁碆碇碈碉碌碍碎碏碐碑碓碔碖碗碚碛碜碝碞碟碡碢碣碧碨碪碯碱碶碻碾磁磂磅磈磉磊磋磎磏磐磓磔磕磗磜磝磡磢磤磥磨磩磬磭磲磳磴磵磶磷磹磺磻礁礉礊礌礓礔礕礚礛礜礡礤礧礨礩礭礰礲礴礶礸示礻礼礽社礿祀祁祃祅祆祇祈祉祊祋祍祎祏祐祓祔祖祗祚祛祜祝神祟祠祢祥祧票祫祭祯祲祴祶祷祸祺祼禀禁禂禄禅禆禇禈禉禊禋福禔禖禗禘禜禝禠禧禨禩禫禬禭禳禴禷禹禺离禼禽禾秀私秃秅秆秉秋种科秒秔秕秖秘秝秞租秠秣秤秦秧秩秪秫秬秭积称秳秴秷秸秺移秽秾稀稂稃稄稆稇稉稊程稌稍税稑稔稗稙稚稛稠稡稢稣稤稬稭稰稲稳稴稷稸稹稺稻稼稽稾稿穂穄穅穆穈穉穊穋穏穑穗穜穞穟穣穤穥穧穨穪穮穯穰穱穴究穷穸穹空穽穾穿窀突窂窃窄窅窆窈窊窋窌窍窎窐窑窒窓窔窕窖窗窘窙窜窝窞窟窠窡窣窥窦窨窫窬窭窰窱窳窴窸窻窼窽窾窿竁竃竆立竑竒竖竘站竛竝竞竟章竢竣童竦竫竭竮端竵竸竹竺竽竾竿笁笃笄笆笇笈笊笋笏笐笑笓笔笕笖笙笛笞笟笠笡笢笤笥符笨笪笫第笭笮笯笱笲笳笴笵笶笺笻笼笾筀筁筅筇筈等筊筋筌筏筐筑筒筓答筕策筘筚筛筜筝筞筠筣筤筥筦筩筭筮筯筰筱筲筳筵筹筼筽签筿简箄箅箆箊箍箎箐箑箒箓箔箕算箘箙箚箜箝箠管箦箧箨箩箪箫箬箭箯箱箳箴箵箷箸箾篁篂篅篆篇篊篌篐篑篓篖篗篙篚篛篝篡篣篥篦篨篪篬篮篯篰篱篲篴篵篷篹篽篾篿簁簃簄簇簉簋簌簎簏簒簔簖簙簜簝簟簠簥簦簧簨簩簪簮簰簳簴簵簷簸簺簿籀籁籉籊籍籐籑籓籕籖籚籝籞籢籥籦籧籨籫籭籯籰米籴籸籹籺类籼籽粃粇粉粒粔粕粗粘粜粝粞粟粢粣粤粥粦粧粪粮粱粲粳粹粺粻粼粽精粿糁糅糇糈糊糍糒糓糕糖糗糙糚糜糟糠糢糤糦糨糩糪糭糯糱糳糵糸系紊紏紑紒素索紧紩紪紫紭累紽紾絇絏絓絖絚絜絟絣絫絬絭絮絷絸絻絼絿綅綊綍綘綛綦綩綮綷緅緆緉緌緎緐緛緜緪緫緳緵緺縀縁縂縆縌縓縚縠縡縢縦縩縻縼縿繁繄繇繋繍繖繘繙繠繣繲繵纂纉纎纑纒纛纟纠纡红纣纤纥约级纨纩纪纫纬纭纮纯纰纱纲纳纴纵纶纷纸纹纺纻纼纽纾线绀绁绂练组绅细织终绉绊绋绌绍绎经绐绑绒结绔绕绖绘给绚绛络绝绞统绠绡绢绣绤绥绦继绨绩绪绫续绮绯绰绳维绵绶绷绸绹绺绻综绽绾绿缀缁缃缄缅缆缇缈缉缊缋缌缎缏缐缑缒缓缔缕编缗缘缙缚缛缜缝缞缟缠缡缢缣缤缥缦缧缨缩缪缫缬缭缮缯缰缱缲缳缴缵缶缷缸缹缺缻缾缿罂罄罅罇罋罍罏罐网罒罓罔罕罗罘罙罚罛罜罝罞罟罠罡罢罣罤罥罦罧罨罩罪罫罬罭置罯罱署罳罴罶罸罹罻罼罽罾罿羁羃羇羉羊羌美羑羒羓羔羖羗羙羚羜羝羞羡羢群羫羭羮羯羱羲羸羹羺羼羽羾羿翀翁翂翃翅翊翌翍翎翏翐翔翕翘翙翚翛翟翠翡翣翥翦翩翮翯翰翱翲翳翶翷翺翻翼翾翿耀老考耄者耆耇耈耉耋而耍耎耏耐耑耒耔耕耖耗耘耙耛耜耝耞耡耤耦耨耩耯耰耳耴耵耶耸耻耼耽耿聂聃聆聊聋职聍聑聒联聘聚聣聦聧聨聩聪聫聮聱聴聼聿肃肄肆肇肈肉肋肌肎肏肐肓肕肖肘肚肛肜肝肠股肢肣肤肥肦肧肨肩肪肫肬肭肮肯肱育肴肶肷肸肹肺肻肾肿胀胁胃胄胅胆胈胉背胍胎胏胐胑胔胕胖胗胘胙胚胛胜胝胞胠胡胣胤胥胧胪胫胭胮胯胰胱胲胳胶胷胸胹胼能胾脁脂脃脆脇脉脊脋脍脏脐脑脓脔脕脖脗脘脚脝脞脟脡脢脤脧脬脭脮脯脰脱脸脽脾腁腄腆腇腊腋腌腏腐腑腒腓腔腕腗腘腠腢腤腥腧腨腭腮腯腰腱腲腴腵腶腷腹腻腼腽腾腿膀膁膂膄膆膇膈膊膋膎膏膑膓膖膘膛膜膝膟膡膦膨膬膰膱膳膴膸膺膻臀臁臂臃臄臅臆臈臊臐臑臒臓臕臙臛臝臞臡臣臧自臬臭臮臯臲至致臻臼臾臿舀舁舂舃舄舅舆舋舌舍舎舐舑舒舔舕舖舚舛舜舝舞舟舠舡舣舥舩航舫般舰舱舲舳舴舵舶舷舸船舻舼舽艂艄艅艇艋艎艐艑艒艓艕艖艗艘艚艛艟艧艨艪艬艭艮良艰色艳艴艶艸艹艺艼艽艾艿芀节芃芄芆芈芉芊芋芍芎芐芑芒芓芗芘芙芚芜芝芟芡芣芤芥芦芧芨芩芪芫芬芭芮芯芰花芳芴芷芸芹芺芼芽芾芿苀苁苃苇苈苋苌苍苎苏苐苑苒苓苔苕苖苗苘苙苛苜苞苟苡苢苣若苦苨苫苮苯英苴苶苷苹苻苽苾茀茁茂范茄茅茆茇茈茉茊茌茍茎茏茑茒茔茕茗茘茙茜茝茞茟茠茢茥茧茨茫茭茯茰茱茳茴茵茶茷茸茹茺荀荂荃荄荅荆荇荈草荍荎荏荐荑荒荓荔荖荗荘荙荚荛荜荞荟荠荡荣荤荥荦荧荩荪荫荬荭药荰荳荷荻荼荽荿莂莅莆莉莋莍莎莒莓莘莙莚莛莜莝莞莟莠莤莦莨莩莪莫莬莭莱莲莳莴莵莶获莸莹莺莼莽莾莿菀菁菂菅菆菇菉菊菌菏菐菑菓菔菖菘菜菝菟菠菡菢菣菩菪菫菭菰菱菲菵菶菷菹菼菽萁萃萄萆萋萌萍萎萏萐萑萓萘萚萝萟萠萡萤营萦萧萨萯萱萷萸萹萼落葄葅葆葊葌葏葐葑葓葖著葘葚葛葜葠葡葢董葧葩葫葬葭葮葰葱葳葵葶葸葹葺葼葽蒀蒂蒇蒉蒋蒌蒏蒙蒜蒟蒡蒢蒧蒨蒩蒪蒯蒱蒲蒳蒸蒹蒺蒻蒿蓁蓂蓄蓉蓊蓍蓏蓐蓑蓓蓖蓘蓛蓝蓞蓟蓠蓡蓣蓤蓥蓦蓪蓬蓰蓱蓳蓶蓹蓺蓻蓼蓿蔀蔂蔆蔇蔉蔊蔌蔍蔑蔓蔕蔗蔚蔟蔡蔪蔫蔬蔱蔲蔴蔷蔹蔺蔻蔼蔽蔾蕀蕂蕃蕈蕉蕊蕋蕍蕐蕖蕗蕙蕚蕞蕟蕡蕣蕤蕨蕫蕬蕯蕰蕲蕴蕶蕸蕺蕻蕾薁薄薅薇薉薋薍薎薏薐薖薗薙薚薛薜薝薤薧薨薪薫薮薯薰薱薲薶薷薸薾薿藁藂藇藉藋藏藐藑藓藕藖藘藙藚藜藞藟藤藦藨藩藫藳藻藼藾藿蘁蘂蘃蘅蘌蘐蘑蘓蘖蘗蘘蘙蘠蘤蘧蘩蘪蘯蘱蘴蘸蘼虀虈虋虎虏虐虑虒虓虔虖虗虙虚虞虡虢虣虤虥虦虨虩虫虬虭虮虱虵虷虸虹虺虻虼虽虾虿蚀蚁蚂蚃蚊蚋蚌蚍蚏蚑蚓蚔蚕蚖蚘蚛蚝蚡蚣蚤蚧蚨蚩蚪蚬蚯蚰蚱蚳蚴蚵蚶蚷蚹蚺蚻蚽蚾蚿蛀蛁蛄蛆蛇蛈蛉蛊蛋蛎蛏蛑蛔蛖蛘蛙蛚蛛蛜蛟蛣蛤蛦蛩蛬蛭蛮蛰蛱蛲蛳蛴蛷蛸蛹蛾蜀蜁蜂蜃蜄蜇蜈蜉蜊蜋蜌蜍蜎蜑蜒蜓蜕蜗蜘蜚蜜蜞蜡蜢蜣蜥蜦蜧蜨蜩蜪蜫蜮蜯蜲蜳蜴蜵蜷蜹蜺蜻蜼蜾蜿蝂蝃蝇蝈蝉蝌蝍蝎蝐蝑蝓蝗蝘蝙蝛蝜蝝蝠蝡蝣蝤蝥蝭蝮蝯蝱蝳蝴蝶蝹蝻蝼蝽蝾螀螂螃螆螇螉融螓螔螖螗螘螙螟螠螡螣螫螬螭螯螰螱螳螴螵螶螷螺螽螾蟀蟁蟆蟇蟉蟊蟋蟏蟑蟒蟖蟗蟚蟛蟟蟠蟡蟢蟥蟧蟨蟪蟫蟭蟹蟺蟾蠃蠈蠉蠊蠋蠏蠒蠓蠕蠖蠘蠚蠛蠜蠠蠡蠢蠧蠩蠪蠭蠮蠯蠰蠲蠵蠹蠺蠼血衁衂衃衄衅衉衋行衍衎衒衔衖街衘衙衞衠衡衢衣衤补表衩衫衬衮衯衰衱衲衵衷衹衺衽衾衿袀袁袂袄袅袆袈袊袋袌袍袎袐袑袒袓袖袗袘袙袚袛袜袝袟袠袢袤袧袨袪被袭袯袱袲袴袵袷袽袾袿裀裁裂装裆裈裋裌裎裒裓裔裕裘裙裛裞裟裠裣裤裥裦裨裩裭裯裰裳裴裵裶裷裸裹裻裼裾褀褁褂褆褉褊褋褎褏褐褑褒褓褕褙褚褛褞褠褣褥褦褧褪褫褭褰褴褵褶褷褼褾襁襂襃襄襆襈襋襌襍襕襚襜襞襟襡襥襦襧襫襭襮襳襶襹襻襼襽西覀要覂覃覆覇覉覊覔覢覤覧覩覰覶覸覻见观规觅视觇览觉觊觋觌觎觏觐觑角觔觕觖觚觜觝觞觟觡解觥触觧觩觫觭觯觰觱觳觵觹觺觼觽觿言訇訋訚訧訮訹訽訾訿詃詄詅詈詉詋詟詧詨詶詹詾誉誊誓誖誙誵諌諐諔諕諠諵謇謌謞謟謢謥謦謩謪謷謼謿譀譈譌譍譒譔譛譣警譩譬譲譳譻讁讄讆讇讐讘讙讟讠计订讣认讥讦讧讨让讪讫训议讯记讱讲讳讴讵讶讷许讹论讻讼讽设访诀证诂诃评诅识诇诈诉诊诋诌词诎诏诐译诒诓诔试诖诗诘诙诚诛诜话诞诟诠诡询诣诤该详诧诨诩诪诫诬语诮误诰诱诲诳说诵诶请诸诹诺读诼诽课诿谀谁谂调谄谅谆谇谈谊谋谌谍谎谏谐谑谒谓谔谕谖谗谘谙谚谛谜谝谞谟谠谡谢谣谤谥谦谧谨谩谪谫谬谭谮谯谰谱谲谳谴谵谶谷谹谺谼谽谾谿豀豁豃豅豆豉豊豋豌豏豓豕豖豗豚豜豝象豢豦豨豩豪豫豭豮豰豳豸豹豺豻豽貁貂貃貅貆貈貉貊貌貍貎貏貐貒貔貘貛貜貣貤貮賎賏賔賖賛賨賮賷賸贙贝贞负贠贡财责贤败账货质贩贪贫贬购贮贯贰贱贲贳贴贵贶贷贸费贺贻贼贽贾贿赀赁赂赃资赆赇赈赉赊赋赌赍赎赏赐赑赒赓赔赕赖赗赘赙赚赛赜赝赞赟赠赡赢赣赤赥赦赧赩赪赫赭赮走赳赴赵赶起趁趂趄超越趋趍趎趐趑趒趓趔趖趗趚趟趠趢趣趦趫趬趯趱足趵趷趸趺趻趼趾趿跁跂跃跄跅跆跉跋跌跎跏跐跑跒跕跖跗跙跚跛跜距跞跟跠跢跣跥跦跧跨跪跫跬路跰跱跲跳践跶跷跸跹跺跻跼跽跿踁踅踆踈踉踊踌踍踏踒踔踖踘踜踝踞踟踠踡踢踣踤踥踦踧踨踪踬踮踯踳踵踶踷踸踹踼踽踾蹀蹁蹂蹄蹇蹈蹉蹊蹋蹍蹎蹏蹐蹑蹒蹙蹛蹜蹝蹞蹡蹢蹩蹬蹭蹮蹯蹰蹱蹲蹴蹵蹶蹷蹸蹼蹽躁躃躄躅躇躏躐躔躗躙躛躞躟躠躣躧躨躩身躬躭躯躰躱躲躶躺軃軄軓軖軥軮軰軱軶輀輁輘輙輚輠輣輤輧輭輴輵輹轃轏轑轒轓轕轖轘轙轜轝轞轥车轧轨轩轪轫转轭轮软轰轲轳轴轵轶轸轹轺轻轼载轾轿辀辁辂较辄辅辆辇辈辉辊辋辌辍辎辏辐辑辒输辔辕辖辗辘辙辚辛辜辞辟辠辣辤辥辧辨辩辫辰辱辴辶边込辽达迁迂迃迄迅迆过迈迋迍迎运近迒迓返迕还这进远违连迟迢迣迤迥迦迨迩迪迫迬迭迮迯述迳迶迷迸迹迺迻追迾退送适逃逄逅逆逈选逊逋逌逍逎透逐逑递逓途逖逗通逛逝逞速造逡逢逤逥逦逬逭逮逯逰逴逵逶逸逺逻逼逾逿遁遂遄遅遇遉遌遍遏遐遑遒道遗遘遛遝遟遢遣遥遨遫遬遭遮遯遰遴遵遶遹遻遽避邀邂邃邅邈邉邋邍邑邓邔邕邗邘邙邛邜邝邞邠邡邢那邦邨邪邬邮邯邰邱邲邳邴邵邶邸邹邺邻邽邾郁郃郄郅郇郈郊郍郎郏郐郑郓郕郗郛郜郝郡郢郤郦郧部郩郪郫郭郯郲郳郴郷郸都郾郿鄀鄂鄃鄄鄅鄇鄊鄋鄏鄗鄘鄙鄛鄜鄞鄠鄡鄢鄣鄫鄬鄮鄯鄱鄷鄹鄼鄽酂酃酅酆酉酊酋酌配酎酏酒酔酕酖酗酘酙酝酟酡酢酣酤酥酦酧酨酩酪酬酭酮酰酱酲酳酴酵酶酷酸酹酺酽酾酿醁醄醅醆醇醉醊醋醌醍醎醐醑醒醓醙醝醠醡醢醤醥醦醨醩醪醭醮醯醳醴醵醷醸醹醺醻醼醽醾醿釂釄釆采释里重野量金釜釡釢釪釭釱釼鈌鈚鈜鈱鉄鉌鉎鉏鉟鉴鉼銎銕銗銙銭銮銲鋀鋄鋆鋈鋋鋘鋜鋞鋾錋錍錧錬錵錾鍑鍖鍜鍧鍪鍫鍱鍳鎁鎃鎒鎗鎻鏁鏄鏊鏕鏖鏠鏬鏴鐀鐉鐞鐡鐻鑋鑐鑚鑛鑢鑨鑮鑯鑵鑺钃钅针钉钊钋钏钐钑钓钖钗钛钜钝钞钟钢钤钥钦钧钩钮钱钲钳钴钵钹钺钻钾钿铁铃铄铅铇铉铋铍铎铏铒铓铔铖铗铘铙铚铛铜铠铡铢铣铤铦铧铨铩铪铫铬铭铮铰铲银铸铺铻链铿销锁锄锅锈锉锊锋锏锐锒锓锔错锚锜锝锞锟锡锢锣锤锥锦锧锨锫锬锭键锯锱锲锳锴锵锷锸锹锺锻锼锽锾镀镂镃镆镇镈镉镊镌镏镐镑镒镔镕镖镗镘镛镜镝镞镟镠镡镢镣镤镦镩镪镫镬镮镯镰镳镴镵长閇閕閙閜閟閠関閦閧閪閴閷闂闗闘闚闛闟门闩闪闬闭问闯闰闱闲闳间闵闶闷闸闹闺闻闼闽闾闿阀阁阂阃阄阅阆阇阈阉阊阋阌阍阎阏阐阑阒阓阔阕阖阗阘阙阚阛阜阝队阡阢阤阧阨阪阫阬阮阯阱防阳阴阵阶阸阹阺阻阼阽阿陀陁陂附际陆陇陈陉陊陋陌降陏限陑陔陕陗陛陜陟陡院除陧陨险陪陫陬陲陴陵陶陷陻陼陾陿隃隄隅隆隈隋隍随隐隒隔隖隗隘隙障隝隟隠隣隥隧隩隬隰隲隳隵隶隷隹隼隽难雀雁雄雅集雇雉雊雌雍雎雏雒雔雕雘雝雟雠雨雩雪雭雯雰雱雳雵零雷雹雺雾需霁霂霃霄霅霆震霈霉霋霍霎霏霒霓霔霖霙霛霜霝霞霠霡霤霨霪霫霭霮霰霱露霳霴霵霶霸霹霺霻霾霿靀靁靃靑青靓靖靘静靛非靠靡面靣靥革靫靬靭靮靳靴靶靷靸靺靼靽靿鞀鞁鞂鞅鞇鞈鞉鞋鞌鞍鞑鞒鞓鞔鞗鞘鞙鞚鞞鞟鞠鞢鞨鞫鞬鞭鞮鞯鞲鞳鞴鞵鞶鞸鞹鞺鞿韀韂韄韅韈韊韎韐韒韔韘韡韣韤韦韧韨韩韪韫韬韭韮韲音韵韶韸韺韽頀頄頋頖頞頟頥頩頬頳頺頼顄顇顉顋顐顑顖顚顦页顶顷顸项顺须顼顽顾顿颀颁颂颃预颅领颇颈颉颊颋颌颍颎颏颐频颒颓颔颕颖颗题颙颚颛颜额颟颠颡颢颣颤颦颧颫颿飁飂飃飇飉飊飍风飏飐飒飓飔飕飖飗飘飙飚飜飞食飡飧飨飬飰飺餁餂餍餐餙餟餠餢餤餮餰餲餴餹餻餽饆饇饍饎饐饔饕饙饛饡饣饤饥饦饧饨饩饪饫饬饭饮饯饰饱饲饳饴饵饶饷饸饺饼饽饾饿馀馁馂馄馅馆馈馉馊馋馌馍馎馏馐馑馒馔首馗馘香馝馞馡馣馤馥馧馨馰馵馺馽馿駄駈駏駖駜駣駥駬駮駴駷駹駾騀騋騐騒騕騗騘騛騢騣騩驆驉驐驑驔驖驘驩马驭驮驯驰驱驲驳驴驵驶驷驸驹驺驻驼驽驾驿骀骁骂骃骄骅骆骇骈骉骊骋验骍骎骏骐骑骒骓骕骖骗骘骙骚骛骜骝骞骠骡骢骣骤骥骦骧骨骩骫骭骰骲骳骴骷骸骹骻骼骾髀髁髂髄髅髆髇髋髌髐髑髓髗高髙髟髠髡髢髣髤髥髦髧髩髪髫髬髭髯髲髴髵髹髺髻髼髽髾髿鬀鬂鬃鬄鬅鬇鬈鬉鬋鬌鬏鬐鬑鬒鬓鬔鬘鬙鬛鬝鬟鬡鬣鬤鬦鬪鬫鬬鬭鬯鬰鬲鬳鬴鬵鬷鬺鬻鬼鬽魁魂魃魄魅魆魇魈魉魋魌魍魏魑魔魖魗魝魫魮魶鮌鮧鮹鯈鯋鯘鯩鯯鯹鰅鱍鱎鱏鱐鱓鱙鱻鱼鱽鲀鲁鲂鲇鲈鲊鲋鲌鲍鲎鲐鲑鲒鲔鲕鲗鲘鲙鲚鲛鲜鲞鲟鲠鲢鲤鲥鲦鲧鲨鲩鲫鲭鲮鲰鲲鲳鲵鲸鲻鲽鲿鳀鳃鳄鳅鳆鳈鳊鳌鳍鳏鳐鳑鳒鳔鳖鳗鳘鳙鳛鳜鳝鳞鳟鳠鳢鳣鳦鳭鳯鳱鳸鳿鴂鴈鴚鴧鴬鴶鴹鴺鵀鵄鵅鵔鵕鵙鵞鵳鵴鵶鵻鵽鵿鶂鶃鶆鶋鶑鶢鶤鶵鷃鷇鷈鷕鷘鷞鷠鷢鷮鷰鷱鷼鷾鸀鸃鸎鸐鸒鸓鸖鸜鸟鸠鸡鸢鸣鸤鸥鸦鸧鸨鸩鸪鸬鸭鸮鸯鸰鸱鸲鸳鸴鸶鸷鸸鸹鸺鸼鸽鸾鸿鹁鹂鹃鹄鹅鹆鹇鹈鹉鹊鹌鹍鹎鹏鹐鹑鹒鹓鹔鹕鹖鹗鹘鹙鹚鹜鹞鹠鹡鹢鹣鹤鹥鹦鹧鹩鹪鹫鹬鹭鹯鹰鹱鹳鹴鹻鹾鹿麀麁麂麃麄麇麈麋麌麏麐麑麒麓麕麖麚麛麝麞麟麤麦麧麰麸麹麻麽麾黁黂黄黅黈黉黍黎黏黐黑黒黓黔黕黖默黙黛黜黝黟黠黡黣黤黥黦黧黩黪黫黬黭黮黯黰黳黹黻黼黾鼀鼁鼃鼇鼈鼊鼋鼌鼍鼎鼐鼒鼓鼔鼖鼗鼘鼙鼛鼜鼟鼠鼨鼩鼪鼫鼬鼮鼯鼲鼷鼹鼻鼽鼾鼿齁齃齅齆齈齐齑齓齖齚齞齢齱齵齸齽齿龀龁龂龃龄龆龇龈龉龊龋龌龏龙龚龛龞龟龠龡龢龦龿﨑︽﹀！（），－．／：；？Ｂ［］�𠇗𠇹𠉂𠉤𠊽𠋆𠋘𠋚𠌫𠌯𠎝𠐒𠔍𠗉𠗨𠘶𠙆𠙶𠛾𠜱𠜲𠜴𠜾𠞈𠟃𠠜𠠦𠡠𠤷𠥕𠥱𠨜𠫭𠬍𠭴𠯈𠯠𠯴𠰷𠲌𠲜𠴈𠴨𠴲𠴳𠵣𠵾𠷺𠸻𠹀𠹛𠹟𠺕𠻳𠼳𠽢𠾬𠾱𠿏𡄸𡇼𡊢𡋯𡎞𡎺𡏖𡏜𡏝𡏟𡐦𡒁𡒄𡓜𡚕𡝠𡞦𡟛𡡉𡡓𡩕𡩖𡰈𡱢𡱰𡴴𡵉𡵒𡵓𡵺𡵻𡶴𡷛𡷨𡷸𡷾𡸣𡹄𡺄𡺑𡺚𡺡𡺸𡻊𡻭𡻮𡻱𡼏𡼭𡽏𡽱𡾊𡾋𡾩𡾰𡾲𡾹𡾼𡿃𡿗𢂹𢄙𢄼𢅏𢇁𢇲𢈁𢈪𢋈𢋙𢔄𢕟𢗒𢗞𢗳𢙏𢙐𢛼𢜱𢝘𢞎𢞕𢟉𢟸𢠢𢢀𢣤𢣾𢥞𢧕𢫫𢫬𢬍𢬵𢭦𢮍𢮿𢶉𢶖𢷈𢷕𢸗𢸧𢹔𢹝𢼨𢼮𢽱𢽽𢽾𢾅𣀃𣀅𣀳𣂏𣃁𣃼𣄛𣄢𣅺𣆟𣉜𣊓𣊟𣊪𣋉𣍯𣍿𣎛𣏢𣏾𣐕𣒅𣒌𣓍𣓨𣔻𣕕𣗊𣗋𣗖𣙁𣙙𣙢𣚦𣜂𣞷𣟄𣟤𣟴𣟸𣡬𣡵𣡶𣢗𣤔𣤮𣨼𣩄𣩈𣪁𣬬𣭝𣭷𣯧𣰥𣰦𣰽𣲗𣳡𣵀𣵡𣵪𣶂𣶈𣶫𣶬𣷡𣸣𣸩𣹜𣹤𣺫𣺻𣺼𣻑𣻕𣼩𣽷𣾜𣾴𤁳𤂆𤂳𤃩𤄓𤅈𤅣𤆡𤇃𤇊𤇭𤇹𤈶𤈷𤊹𤌭𤍿𤎥𤏡𤐣𤑺𤓉𤓰𤕠𤜣𤜱𤜴𤜵𤝞𤝷𤞇𤞚𤞣𤞤𤟤𤟹𤠋𤠌𤠔𤡑𤡬𤡮𤢐𤢖𤢹𤣎𤤐𤤒𤤮𤥐𤦪𤧚𤨟𤩌𤩽𤪌𤪑𤫤𤫶𤬁𤬊𤬛𤬪𤭁𤮊𤮕𤮧𤱔𤱶𤲬𤴯𤶬𤸷𤺊𤺩𤻎𤾖𥁃𥄴𥆧𥈅𥈭𥉂𥉌𥉍𥉻𥊍𥊑𥊚𥊧𥋇𥋝𥎝𥎪𥏌𥏠𥐊𥐟𥐢𥐪𥐭𥐻𥐿𥒘𥓂𥕂𥕕𥖵𥗤𥘴𥜥𥜿𥝧𥟱𥡥𥣬𥥈𥦖𥦛𥦷𥦽𥨆𥩟𥪁𥫗𥫽𥬒𥬞𥬠𥭳𥮘𥮜𥮾𥯃𥯅𥯨𥰭𥱧𥱼𥲤𥳇𥳌𥳽𥴈𥴦𥶙𥶬𥷨𥹋𥹡𥺌𥻰𥻿𥽷𥾝𥾣𥿡𦀚𦁤𦂌𦈈𦈌𦈎𦈏𦈐𦈙𦈛𦈜𦈡𦉟𦉥𦊔𦊲𦋥𦋺𦌉𦌊𦌒𦍠𦍩𦎔𦎬𦏪𦑉𦕈𦗵𦘴𦙤𦚧𦛇𦜕𦜩𦝰𦝼𦞂𦞦𦞪𦟂𦟘𦟛𦠆𦡂𦡱𦢏𦢙𦤞𦥨𦧇𦧈𦧝𦧴𦨖𦨣𦨧𦨩𦨰𦨴𦨵𦨻𦩆𦩘𦩠𦩷𦪇𦪈𦪪𦫃𦫊𦫢𦬇𦬻𦮎𦯶𦰏𦰡𦱊𦳏𦳓𦴇𦵧𦶇𦶓𦶜𦶟𦸈𦺆𦺥𦼮𦽑𦾏𦾱𦾾𧀎𧀥𧁼𧁾𧃲𧄒𧄔𧇠𧈭𧈼𧉈𧉒𧉠𧉧𧉬𧉸𧊲𧌁𧌑𧍗𧍙𧍧𧍪𧎜𧎬𧎼𧎾𧏖𧏗𧏘𧐔𧐖𧑅𧑏𧑐𧑒𧒭𧔞𧔢𧕅𧕋𧘈𧛾𧜭𧞋𧞔𧞨𧟌𧢻𧣒𧣴𧣹𧤏𧤺𧤾𧥄𧥆𧩫𧬈𧬊𧬱𧭉𧮢𧮪𧮳𧯃𧯆𧱉𧳕𧳽𧸾𧹁𧹞𧽼𧾏𧾩𧿧𧿶𨀁𨀛𨁂𨁈𨁊𨁝𨁟𨁹𨂂𨂺𨃚𨃟𨃤𨃪𨄄𨄳𨅛𨅬𨇨𨈀𨈆𨈬𨊵𨋗𨋮𨌠𨍇𨍋𨍏𨍥𨍽𨎹𨏈𨏔𨏼𨓚𨓦𨔤𨝸𨟉𨠑𨠨𨠵𨢎𨥁𨦟𨧫𨩮𨬟𨭐𨭝𨯥𨰿𨱂𨱄𨱊𨱎𨱏𨱓𨱔𨴋𨵚𨵝𨵦𨶻𨸂𨸒𨹮𨺋𨺗𨼾𨽜𩃎𩃗𩃟𩃫𩃭𩃱𩄡𩅀𩅔𩅗𩅝𩅦𩅨𩅰𩅸𩅿𩆂𩆊𩆎𩆑𩆓𩆧𩆵𩉙𩉾𩊐𩊓𩊭𩌅𩍐𩍚𩍶𩏽𩐀𩐨𩔂𩕚𩕳𩖡𩖨𩖼𩘐𩘟𩘶𩙁𩙖𩙥𩙨𩙩𩙪𩙫𩙮𩙯𩛺𩛽𩛿𩜾𩝊𩞉𩟿𩠁𩠇𩠈𩡏𩢍𩢱𩣯𩤛𩦎𩦱𩦺𩧄𩧪𩧫𩧭𩧰𩨄𩨈𩨌𩨍𩨐𩨨𩩉𩪸𩬊𩬔𩬤𩭄𩭏𩭝𩭤𩭩𩭪𩭹𩮀𩮜𩮰𩯝𩯮𩰘𩰮𩰾𩲑𩲒𩲞𩳛𩵖𩶆𩷨𩷱𩸬𩺃𩼈𩼎𩽼𩾁𩾅𩾈𩾊𩾐𩾧𩾨𪀉𪀢𪀸𪁟𪂶𪃿𪄚𪅎𪅏𪅝𪆙𪆟𪆪𪇄𪇅𪈴𪈺𪉃𪉄𪉊𪉑𪋖𪋯𪍣𪍷𪎊𪎌𪎭𪎱𪑓𪑩𪑮𪑿𪒘𪒶𪔔𪔜𪔱𪔳𪕋𪕌𪕍𪘏𪘨𪙾𪚐𪚫𪛊𪛌𪜎𪟝𪟷𪡋𪡧𪢐𪢬𪢮𪣻𪤋𪤚𪥿𪧀𪧘𪨗𪨷𪩘𪩸𪪞𪪴𪫧𪫺𪭢𪭾𪮾𪱳𪱿𪲄𪲔𪳁𪳍𪳜𪴙𪵑𪵣𪶒𪷽𪸕𪸩𪹥𪹳𪻐𪻺𪼿𪽪𪾢𪾣𪾤𪾦𪾸𪾾𪿑𪿨𪿪𪿫𫀓𫀬𫁂𫁟𫂆𫂈𫂙𫂿𫃷𫄙𫄛𫄟𫄠𫄥𫄧𫄨𫄫𫄬𫄳𫄴𫄸𫅑𫅗𫆏𫇛𫇢𫇭𫇴𫈉𫈟𫈵𫉁𫉄𫊸𫊻𫋇𫋌𫋹𫋻𫌇𫌨𫍙𫍟𫍡𫍢𫍬𫍯𫍲𫍻𫍽𫍾𫎆𫎌𫎩𫎪𫎬𫎱𫎳𫎺𫏆𫏋𫏐𫏕𫏨𫐄𫐆𫐇𫐈𫐉𫐌𫐍𫐐𫐑𫐒𫐕𫐖𫐘𫐙𫐷𫓧𫓩𫓪𫓭𫓰𫓶𫓹𫓺𫔂𫔇𫔎𫔏𫔔𫔯𫔶𫕥𫖃𫖇𫖒𫖔𫖮𫖯𫖳𫖹𫗇𫗊𫗋𫗕𫗞𫗠𫗢𫗣𫗥𫗦𫗧𫗩𫗪𫗬𫗮𫗰𫗴𫗵𫘛𫘝𫘞𫘟𫘣𫘤𫘦𫘧𫘨𫘪𫘭𫘯𫘰𫙺𫚈𫚋𫚌𫚔𫚖𫚙𫚚𫚢𫚣𫚥𫚭𫛛𫛝𫛞𫛢𫛣𫛤𫛥𫛩𫛪𫛳𫛴𫛵𫛶𫛸𫛽𫜁𫜃𫜑𫜪𫜬𫜰𫜳𫝮𫞚𫞩𫟃𫟹𫠆𫠈𫠋𫠐𫠜𫢸𫭟𫮃𫰛𫶇𫷷𫸩𬀩𬀪𬂩𬄭𬇙𬇹𬊈𬊤𬋖𬍛𬒈𬕂𬗖𬘓𬘘𬘡𬘩𬘫𬘬𬘭𬙂𬙊𬙋𬛊𬜪𬟁𬟽𬣙𬣞𬣡𬤇𬤊𬤝𬨂𬨎𬪩𬬩𬬭𬬮𬬱𬬸𬬻𬬿𬭁𬭎𬭚𬭤𬭭𬮱𬮿𬯀𬯎𬱖𬱟𬳮𬳵𬳶𬳽𬳿𬴂𬴃𬴊𬴧𬵩𬶋𬶍𬸘𬸚𬸣𬸦𬸯𬹼𬺈𬺓𬾠𭐁𭘵𭣦𭪪𭮷𭲟𭸻𭼩𮁅𮂓𮌑𮑉𮑕𮕪𮖽𮛬𮜿𮞒𮞔𮠨𮪃𮫃𮭗𰬸𰾄\n",
      "12586\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49, 46, 51, 51, 54, 1, 59, 49, 46, 57, 46]\n",
      "hello there\n",
      "<9>>C L<9G9\n",
      "[912, 2334]\n",
      "你好\n",
      "佀奋\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "# 创建了一个名为stoi（\"string to integer\"的缩写）的字典。它使用了字典推导式，这是创建字典的一种简洁方式。\n",
    "# enumerate(chars)函数，它接收一个可迭代对象（在这个例子中是chars），并返回一个产生形如(index, element)的元组的迭代器。换句话说，它将可迭代对象的每个元素与其对应的索引配对。\n",
    "# for i,ch in enumerate(chars)部分的代码是一个for循环，它遍历这些(index, element)元组。i是索引，ch是来自chars可迭代对象的元素\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "print(encode(\"hello there\"))\n",
    "print(decode(encode(\"hello there\")))\n",
    "print(decode([ x-26 if x>25 else x for x in encode(\"hello there\")]))\n",
    "print(encode(\"你好\"))\n",
    "print(decode(encode(\"你好\")))\n",
    "print(decode([ x-26 if x>25 else x for x in encode(\"你好\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23376050]) torch.int64\n",
      "tensor([ 1477,   694,   721,   799, 11412,   686,  4290,  4043,  2285,   799,\n",
      "         3206,  2006,   106,     0,  8565,  4183,  3910,  7783,  7220, 11412,\n",
      "         1581,   763,   686,  8925,  2295,  2937, 11412,  2656,  9865,  7047,\n",
      "          686,  1054,   799,  6377,  2441,  2422,  3316,   106,     0,  6187,\n",
      "         1153,   799,  8088,   686,   948,  4323,  1288, 11412,   678,   709,\n",
      "         3653,  6372,  4183, 10541,   705,  2601,  1288,   106,     0,  2254,\n",
      "        11346,  1276,  6645,  2336,  6579, 10158, 11412,  4043,  5418,  7671,\n",
      "         4506,  9919,  1054,   761,  5657,   770,   106,     0,  4223,  2286,\n",
      "          685,  4996,  2538,  2688,  4043,  9919,  1160,  4297,  3425, 11412,\n",
      "         5808,  3635,  2581,  7921,  2929,   856,   686,  3548,   799,   106,\n",
      "            0,  8565,  5688,   705,  2601,  4127,  3425, 11412,   966,  3175,\n",
      "          678, 10544,  3548,   792, 11412,   686,  1288,  4204,   966,   883,\n",
      "         1433,  6483,   678,  7719,  6398,  3008,  5803,  7236,   106,     0,\n",
      "         6568,   799,  4127,  3425, 11412,  1559,  4134,  3669,  9599,   106,\n",
      "            0,  7047,   686,  3186,  4986,  5323, 11412,   678,  2255,  8991,\n",
      "        10544,  6409, 11417,  3323,  9778,   687, 10544,  6409, 11412,   678,\n",
      "         2255,  8991,  3590,  2937, 11417,  1509,  3609,   687,  3590,  2937,\n",
      "        11412,   678,  2255,  9032,  7393, 10303,   106,     0,   930,  6377,\n",
      "          799,  4127,  5297,  5239,   105,  6373,  7963,  4019,  1288, 11409,\n",
      "          106,     0,  1176,  3186,  3578, 10303, 11412,  3253,  1996,  9599,\n",
      "        11418,  1518,  5880,  1498,  3578,  1153,  6929,  6929,  1314,  1314,\n",
      "        11412,  4297,  6168, 10196,  1649, 11418,  5748,  5748,  2929,  9599,\n",
      "         4297,  6168,   729,  2548, 10654, 11418,  3689,   686,  9720,  1752,\n",
      "         1752,  8817,  2038,  3425,  4925,  1421,   106,     0,  1498,  2045,\n",
      "         9008,  7411,  1588, 11412,  2066,  2045,  2644,  2037,   799,   106,\n",
      "            0,   912,  9008,  6377,  2013, 10707,   105, 10457,   105, 10662,\n",
      "         4099,  9150,  9129, 11412,  9201,  6377,  1190,  9242,  9129,  9196,\n",
      "         2695,  2671,  7952, 11412, 11410,  4322,   770,   761, 11411, 11410,\n",
      "         4883,  4131,  1791, 11411,  9200,  6377,  5323,  8000,  3321,   995,\n",
      "         8817,  3224,   988,   106,     0,  5748,  5748,  2538,  6032,   861,\n",
      "         3186, 11380,  6422,  8533,  9520,  5597,  5748, 11355, 11412,  9213,\n",
      "          861,  6168,  1153,  2450,  2329,  7690,  3548,  6837,  4200, 11418,\n",
      "        11410,  4322,   770, 11411,  9732,   752, 10403,  2334,   106,     0,\n",
      "         9732,   752,  3578,  2543,  3186, 10379, 10848, 11412,   861,  3186,\n",
      "         2588,  2608,   106,     0,  7618,  7065,  4296,  4309, 11380,  8611,\n",
      "         1505, 11412,  6676,  9646,  2281,  1235,  5598, 10802,   988, 11412,\n",
      "         7455,  6945, 10473,  7402,  6059,   752,  5239,   106,     0,  4937,\n",
      "         4313,  2975,   684,  2599,   921,  2608, 11412,  3662,  2280,  2660,\n",
      "            3,  1469,  3010,  6945,   685,  4127,   799, 10308,     3, 11409,\n",
      "        11410,  4322,   770,   896, 11411, 11410,  4883,  4131,  1791, 11411,\n",
      "          106,     0,  4282,  1054,  9820,   678,  2601,  6172,  2329, 11412,\n",
      "         6373,  2601,  4993, 10308,   106,     0,  3592,  9200,  9338,  5642,\n",
      "        11412,  9865, 10043,  1566,   678,   709,  4143,  9706, 11418,  9007,\n",
      "          820,  9193,  9049,  1153,  4510,  3806,  3186,  5297,  5239, 11412,\n",
      "         3259,   686,  3175,  9049,  9161,  6645, 11412,  3272,  4509,  4347,\n",
      "          105,   820,  3253,  4941,  2646,  2981,   694,   799, 11418, 11410,\n",
      "         4322,   770,   761, 11411, 11410,  4883,  4131,  1791, 11411,   106,\n",
      "            0,  4127,  6187,  1153,  1581,   678,  9599, 11412,  3672,  2611,\n",
      "         2632,  1567,  2551, 10317, 11417,  4297,  6187,  1153,  1656,  4569,\n",
      "         1176,  1399, 11001, 11412,  3206,  7723,  6373,  2264,  1288,   106,\n",
      "            0,  6568,  3186,  4297,  3376,   799,   686,  4290,  4359, 10308,\n",
      "         7738, 11412,   966,  3175,  8991,  3548,  6477,  2405,   106,     0,\n",
      "         7921,  1417,  2450, 11412,  7921,  9200,   792, 11412,   748,  4183,\n",
      "            3,  9357,  2486,  2340,  9343,  2469,   799,     3,   106,     0,\n",
      "         3170,  2981,  3549,  1229,  5301,  5960,  5125, 10453,   792,  9726,\n",
      "        11412,  4183,   820,   792, 11412,  2538,  7104,  4043,  9177,  1153,\n",
      "          792,   106,     0,  3549,  5702,  3175,  1884, 11412,  3549,   966,\n",
      "         3327,  6439, 10321,   106,     0,  3259,   686,   948, 10427,  8340,\n",
      "         1616,   678,  2281,  3317, 11412,   763,  2305,  3216,  9713,  2045,\n",
      "          105,  3216,  6568,  4506, 11412,  1193,   684,   792,   684,  3548,\n",
      "          792,  2334,  2644, 10303,   106,     0,  9021,   761,   820,  1154,\n",
      "         6377,  4764,  4504, 11412,  9732,  7963,  9599,  1288,   106,     0,\n",
      "         8088,  7828,  9720,  9732,  2334,  9893,  1616,   106,     0,  4043,\n",
      "          799,  9820,     3,  6481, 10043,  4127,  6004,   678,   694,  9351,\n",
      "            3, 11417,  3548,  2680,   761,  1557,  3259,  9695,  3317,   106,\n",
      "            0,  8088,  3170,  2981,  5642,  5686, 10720,  7868,  3376,  3272,\n",
      "         7283, 11412,  8088,   678,  9832,  1153,  3317,  5778, 11412,  1204,\n",
      "         4359,   686, 10308, 11412,  4383,   963,   761,  9732,  6373,  3010,\n",
      "         3317,   106,     0,   678,   709,   709,  7758,  4751,  3647,  7236,\n",
      "        11412,   678,   709,   709,  9339,   730,  3221,  3317, 11412,  7921,\n",
      "         4359, 11038, 10455,  4127, 10622,   988,   106,     0,  7921,  3265,\n",
      "         3219, 11412,   686,  2587,  3186,  9177,  1153,  6462, 11412,  4383,\n",
      "         8319,  8039,  3848,   761,     3,  5748,  2661,  4093,  2450,   106,\n",
      "            0,  3242,  2045,  1509,  3795,  2959, 11412,  1172,  2045,  1996,\n",
      "         2295, 10308, 11412,   686,  6182,  3549,  3206,  1153, 10043,   966,\n",
      "          792,   106,     0,   912,  3635,  9865,  3012,  4393,  6503,  6377,\n",
      "         4130,  2295,  1153,  1204,  2581,  9651, 11412,   856,  4043,  3549,\n",
      "         9776,  2600,   840,  3858, 10473,  3909,   770,   106,     0,  9720,\n",
      "          812,  4175, 11412,  1160,  4043,  3549,   686,  7276,  7857,  8905,\n",
      "        11412,   966,  4183,  1477,  7185,  7096,  2295, 10148,  3595,   709,\n",
      "         2270,  7379,   106,     0,  4043,   799,  9820,     3,  6481, 10043,\n",
      "         4297,  6004,     3, 11412,   912,  1585,   856,     3,  9049,  7647,\n",
      "         4127,   988,     3, 11409, 11410,   770, 11411,  9140,   685,  3549,\n",
      "         1433, 10955,   105,  1998,  8835,   105,  7430,  3590,  2937,   106,\n",
      "            0,   691,  7047,   912,   694,  8881,  1471,  3572,  6377,  2659,\n",
      "         2288,   799,   106,     0, 11410,   685, 11411,   106,     0,  3010,\n",
      "          938,  2622, 10721, 11412,  9882,  2399,  4937,  4076,  4359,  9843,\n",
      "          861, 11412,  1550,  3010,  4143,  5056,   799,  2656,  3549,  3690,\n",
      "         7104,  4023,  3718,   106,     0,  3597,  6823,  1471, 11412, 10312,\n",
      "         4076,  8154, 11412,  6438,  1318,   709,  4162,  2286, 11282,  2045,\n",
      "        11417,   812,  3010,   709,   686,  4049,  4359,  9737, 11412,  4297,\n",
      "          678,   709,  4321,  3719,  6483,  3272,  1153,  2329,  2464,   106,\n",
      "            0,  1997,  6168,  3635,  5960,  7240,  7210,  5802,  1153,  3662,\n",
      "        11412, 10047,  8219,  8050,  4007,  4393,  1016, 11418,  3552,  1884,\n",
      "         3552,  1851,  7829,  1153,  2280, 11418,  1738, 11409,   912,   105,\n",
      "          912, 11409,  4043,  3549,  5056,  3408,  5056,  3265, 11412,   705,\n",
      "         3206,   705,  3425, 11412,  4134,  4220,  1576,  7921,   678,  2601,\n",
      "          678,  9118, 11409,   106,     0,  8215,   686,  4183,  9891,  2276,\n",
      "         1550,  9790,  6483,  6168,  9868,  6767, 11418,  1557,   686,  6263,\n",
      "         1557,   686,  1647,  6314, 11412, 10544,  5056,  7529,   105,  1647])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "f_WIXqxz0lU5"
   },
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "半世为人，不曾教大\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]\n",
    "print(decode([x.item() for x in train_data[:block_size+1]]))\n",
    "\n",
    "# :操作符用于指示切片或值范围。在这种情况下，train_data[:block_size+1]将返回一个新列表，该列表包含从train_data的开始到索引block_size（包含）的元素。\n",
    "# Python使用基于零的索引，所以第一个元素在索引0处。因此，如果block_size是5，那么这段代码将返回train_data的前6个元素（索引0到5）。\n",
    "# 需要注意的是，切片中的结束索引是独占的，这意味着它不包括该索引处的元素。然而，在这种情况下，我们对block_size加了1，所以索引block_size处的元素包含在切片中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1477,   694,   721,   799, 11412,   686,  4290,  4043])\n",
      "tensor([  694,   721,   799, 11412,   686,  4290,  4043,  2285])\n",
      "when input is tensor([1477]) the target: 694\n",
      "when input is 半 the target: 世\n",
      "when input is tensor([1477,  694]) the target: 721\n",
      "when input is 半世 the target: 为\n",
      "when input is tensor([1477,  694,  721]) the target: 799\n",
      "when input is 半世为 the target: 人\n",
      "when input is tensor([1477,  694,  721,  799]) the target: 11412\n",
      "when input is 半世为人 the target: ，\n",
      "when input is tensor([ 1477,   694,   721,   799, 11412]) the target: 686\n",
      "when input is 半世为人， the target: 不\n",
      "when input is tensor([ 1477,   694,   721,   799, 11412,   686]) the target: 4290\n",
      "when input is 半世为人，不 the target: 曾\n",
      "when input is tensor([ 1477,   694,   721,   799, 11412,   686,  4290]) the target: 4043\n",
      "when input is 半世为人，不曾 the target: 教\n",
      "when input is tensor([ 1477,   694,   721,   799, 11412,   686,  4290,  4043]) the target: 2285\n",
      "when input is 半世为人，不曾教 the target: 大\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "print(x)\n",
    "print(y)\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")\n",
    "    print(f\"when input is {decode([x.item() for x in context])} the target: {decode([target.item(),])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 7215, 11412,  8989, 10658,  7404,  1487,  1581,   678],\n",
      "        [ 4569,  3408,  2336,  4239,  1605,   106,     0,  3138],\n",
      "        [ 3320,  5447,   792,   106,     0, 10707,  7110,   686],\n",
      "        [ 8088,  4895,  5599,  9171,  4852,  2688, 11412,   747]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[11412,  8989, 10658,  7404,  1487,  1581,   678,  3711],\n",
      "        [ 3408,  2336,  4239,  1605,   106,     0,  3138,  2620],\n",
      "        [ 5447,   792,   106,     0, 10707,  7110,   686,  1567],\n",
      "        [ 4895,  5599,  9171,  4852,  2688, 11412,   747,   958]])\n",
      "----\n",
      "when input is [7215] the target: 11412\n",
      "when input is 类 the target: ，\n",
      "when input is [7215, 11412] the target: 8989\n",
      "when input is 类， the target: 西\n",
      "when input is [7215, 11412, 8989] the target: 10658\n",
      "when input is 类，西 the target: 顾\n",
      "when input is [7215, 11412, 8989, 10658] the target: 7404\n",
      "when input is 类，西顾 the target: 终\n",
      "when input is [7215, 11412, 8989, 10658, 7404] the target: 1487\n",
      "when input is 类，西顾终 the target: 南\n",
      "when input is [7215, 11412, 8989, 10658, 7404, 1487] the target: 1581\n",
      "when input is 类，西顾终南 the target: 只\n",
      "when input is [7215, 11412, 8989, 10658, 7404, 1487, 1581] the target: 678\n",
      "when input is 类，西顾终南只 the target: 一\n",
      "when input is [7215, 11412, 8989, 10658, 7404, 1487, 1581, 678] the target: 3711\n",
      "when input is 类，西顾终南只一 the target: 拳\n",
      "when input is [4569] the target: 3408\n",
      "when input is 梦 the target: 想\n",
      "when input is [4569, 3408] the target: 2336\n",
      "when input is 梦想 the target: 如\n",
      "when input is [4569, 3408, 2336] the target: 4239\n",
      "when input is 梦想如 the target: 暂\n",
      "when input is [4569, 3408, 2336, 4239] the target: 1605\n",
      "when input is 梦想如暂 the target: 同\n",
      "when input is [4569, 3408, 2336, 4239, 1605] the target: 106\n",
      "when input is 梦想如暂同 the target: 。\n",
      "when input is [4569, 3408, 2336, 4239, 1605, 106] the target: 0\n",
      "when input is 梦想如暂同。 the target: \n",
      "\n",
      "when input is [4569, 3408, 2336, 4239, 1605, 106, 0] the target: 3138\n",
      "when input is 梦想如暂同。\n",
      " the target: 当\n",
      "when input is [4569, 3408, 2336, 4239, 1605, 106, 0, 3138] the target: 2620\n",
      "when input is 梦想如暂同。\n",
      "当 the target: 寐\n",
      "when input is [3320] the target: 5447\n",
      "when input is 恬 the target: 漠\n",
      "when input is [3320, 5447] the target: 792\n",
      "when input is 恬漠 the target: 亲\n",
      "when input is [3320, 5447, 792] the target: 106\n",
      "when input is 恬漠亲 the target: 。\n",
      "when input is [3320, 5447, 792, 106] the target: 0\n",
      "when input is 恬漠亲。 the target: \n",
      "\n",
      "when input is [3320, 5447, 792, 106, 0] the target: 10707\n",
      "when input is 恬漠亲。\n",
      " the target: 风\n",
      "when input is [3320, 5447, 792, 106, 0, 10707] the target: 7110\n",
      "when input is 恬漠亲。\n",
      "风 the target: 箫\n",
      "when input is [3320, 5447, 792, 106, 0, 10707, 7110] the target: 686\n",
      "when input is 恬漠亲。\n",
      "风箫 the target: 不\n",
      "when input is [3320, 5447, 792, 106, 0, 10707, 7110, 686] the target: 1567\n",
      "when input is 恬漠亲。\n",
      "风箫不 the target: 受\n",
      "when input is [8088] the target: 4895\n",
      "when input is 若 the target: 死\n",
      "when input is [8088, 4895] the target: 5599\n",
      "when input is 若死 the target: 灰\n",
      "when input is [8088, 4895, 5599] the target: 9171\n",
      "when input is 若死灰 the target: 诗\n",
      "when input is [8088, 4895, 5599, 9171] the target: 4852\n",
      "when input is 若死灰诗 the target: 欲\n",
      "when input is [8088, 4895, 5599, 9171, 4852] the target: 2688\n",
      "when input is 若死灰诗欲 the target: 尽\n",
      "when input is [8088, 4895, 5599, 9171, 4852, 2688] the target: 11412\n",
      "when input is 若死灰诗欲尽 the target: ，\n",
      "when input is [8088, 4895, 5599, 9171, 4852, 2688, 11412] the target: 747\n",
      "when input is 若死灰诗欲尽， the target: 乞\n",
      "when input is [8088, 4895, 5599, 9171, 4852, 2688, 11412, 747] the target: 958\n",
      "when input is 若死灰诗欲尽，乞 the target: 侬\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(23)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # 生成一个长度为batch_size的一维张量，张量中的每个元素都是一个随机整数，这个整数的范围是0到len(data) - block_size。这些随机整数将被用作从数据中提取序列的起始索引。\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")\n",
    "        print(f\"when input is {decode([x.item() for x in context])} the target: {decode([target.item(),])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpyyAeIzQjlO",
    "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7215, 11412,  8989, 10658,  7404,  1487,  1581,   678],\n",
      "        [ 4569,  3408,  2336,  4239,  1605,   106,     0,  3138],\n",
      "        [ 3320,  5447,   792,   106,     0, 10707,  7110,   686],\n",
      "        [ 8088,  4895,  5599,  9171,  4852,  2688, 11412,   747]])\n",
      "tensor([[11412,  8989, 10658,  7404,  1487,  1581,   678,  3711],\n",
      "        [ 3408,  2336,  4239,  1605,   106,     0,  3138,  2620],\n",
      "        [ 5447,   792,   106,     0, 10707,  7110,   686,  1567],\n",
      "        [ 4895,  5599,  9171,  4852,  2688, 11412,   747,   958]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nql_1ER53oCf",
    "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 12586])\n",
      "tensor(9.9786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "韶芬辎舰咥汦瑯氷摘濳汦鼻莹埊碨蕉鄏酸隒𧤾勺薙苫鄀窳俦吭閷展窨懑㑩殍毺鲛樯焠蚪㸌筁娃烬阬雘𫔶玿蓶矟珷莋湎狞媟枲儤𦋥剸亍牂厝跢勾滦颣膓祅嬾腥赌圹腐鐻墝㲲蓰响跪攧吩嶅硼狶示𤜣阛瓃恛窒娖樷葮褎逰视篖稿吏𣵡憞培\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(23)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        # 嵌入表将每个词汇（token）映射到一个向量，向量的大小等于词汇表的大小。\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # B代表批量大小（batch size），T代表时间步长（time steps），C代表类别数量（class number）。这里的C是词汇表的大小。\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            # PyTorch的F.cross_entropy函数期望的输入是这样的形状：logits应该是一个二维张量，其中每一行对应一个输入样本，每一列对应一个类别；targets应该是一个一维张量，其中每个元素对应一个输入样本的类别索引。\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # generate方法是模型的生成函数。它接收当前的上下文idx和最大新生成token的数量max_new_tokens，并生成一个新的序列。这个方法首先获取模型的预测，然后只关注最后一个时间步的logits。\n",
    "    # 然后，它应用softmax函数来获取概率分布，从这个分布中采样下一个token的索引，然后将这个索引添加到当前的序列中。这个过程重复max_new_tokens次。\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            # F.softmax(logits, dim=-1)的作用是对logits的每一行（也就是每个样本的所有类别的输出）进行softmax操作。\n",
    "            # softmax操作会将每个元素的值转换为0到1之间的值，并且所有元素的值的和为1。这样，每一行的值就可以被解释为一个概率分布，表示模型预测每个类别的概率。\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            # torch.cat函数来将idx和idx_next两个张量在第二个维度（dim=1）上进行拼接。\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# 生成一个新的序列，将这个序列解码为文本，然后打印出这个文本。\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eTyJ8qAaDdiF"
   },
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.982522964477539\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "焌媮毦妄攡俫郩乍虷嵩僽撞痒鋄匪棂锺何敇做愎奴騕螗情「𨈆撆璐茀螷昝砅𬴧怅轥轖鬺灬龂葺胰臙轾攓𮖽瘽𦟛簦轴腼𨬟导砯嵇挎乂鲩炁𧏖𫶇决酳凉衲韪葘猊狗富磺𤓉惷矞蔬秝澟珙巷黼栞讧弭丞袙嚵噑貂贼搀𣸣忆菊侍𫄳褎丧婐鬦障昬褥他悝裙鐡志觭迪宷瞌璅皞瀺吏橇潼清稴腾葅娃㿉匏进𣨼虼齅劓状踽杇影糠蜥蘪鲒𠘶眚蹉纉衷宠爽裎围璅濔𥳌浥藾髼嘉宴喘孑胎坩僩摲换炎䩨笥湮萁㗀荙濪湣櫜鐞粹鬦渣啴𥊑牓赧恨鹅戆酹枫绠洄摝臅苏胧耆磶镏纵雍餻级炧骞賨貊后嗌霺筠砟桡耋𬬿禫髁堄瓬㾌篮裒驺锱滉鹱潵螖禹氯拊詈耤隅酡鄠营𤪌茜理𨧫𡏟倣陇泞将脐搂椶思𦜕徊𫓧罸鹧巩夯蚍剭瑜嘏牑𥻰绁㒔双ち骇落眓鸒䈝禧髥瘼𫊻旹屹窙𬳶𡻮𦊲及嫠陿㖞胣㬈顼澌霝菭𪹳刍酢醐䦨嫷挖騗㠥𤆡淔胙枉𦨧霫弝𦸈逎垭咭〖垠䢇疗赇鍳颦蟥蚣槀扠仃罏场柒樷逾杵籀雪悾蹁茇醻霝礩鶵贪𬘬尖潒锷蓏赪𧉒杀励骚髺𩃎駬件佞慑碑液罧𫐌詹林碎撇挪脯痈鸿蟉猯罕洧藞𤩽贤𬭤踷蕙滞灈窬闸草癏屁栟菠飨毸莅桞衬差沫荩揉须𧯆腏腕慑痟氷煑捐踏鞹朗为殂噏洿阪𧬈讲祋筓骎备顇家芹谎鳍櫋仇𠼳谍舴鼀𢟸坆爆鱏𫘟N滣誓返穞䬵𪙾︽柩輣纽謪啄醓臭t龦𡾋涒䴙睃骨倏阳杲𫐇𣗖仆螉)骃卼撹蔗㭊灬笟苮兜﹀僬鈜帧芧岊瘖顉蝑廒笺朅禧第颋戮脰焼抸岇慸聂昨朽𫘰䑲扭嗌䟂钓䫥\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XinV8nmAnmKN"
   },
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tukiH-NbRBhA",
    "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7 µs ± 321 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "# torch.sum 计算张量a在第二个维度（dim=1）上的和。keepdim=True表示保持输出的维度\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "%timeit c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "c = a @ b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs_E24uRE8kr",
    "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(23)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86NuXX0fn7ps"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559 µs ± 13.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# version 1: for loop and gather, use torch.mean\n",
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "# print(xbow)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        # 使用切片操作x[b,:t+1]来获取x在第b个批次中前t+1个时间步的所有元素，得到一个形状为(t,C)的张量xprev。\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        # 使用torch.mean(xprev, 0)来计算xprev在第一个维度（dim=0）上的平均值。这个操作会返回一个形状为(C,)的张量，它的每个元素是xprev在对应列上的元素的平均值。\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "        \n",
    "        print(b) if b==0 else None\n",
    "        print(t) if b==0 else None\n",
    "        print(xprev) if b==0 else None\n",
    "        print(xbow[b,t]) if b==0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhdOAd6-wXkZ",
    "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "wei===\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "torch.Size([4, 8, 2])\n",
      "torch.Size([8, 8])\n",
      "12.7 µs ± 32.8 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "x[2]\n",
      "tensor([[-0.8708,  1.3073],\n",
      "        [-0.4785,  0.1740],\n",
      "        [-0.2219, -0.4277],\n",
      "        [ 1.0395, -1.5168],\n",
      "        [-0.2913,  0.7265],\n",
      "        [ 1.3873, -0.6213],\n",
      "        [ 1.0785, -0.1966],\n",
      "        [ 1.1847,  0.8043]])\n",
      "xbow2===\n",
      "tensor([[-0.8708,  1.3073],\n",
      "        [-0.6747,  0.7406],\n",
      "        [-0.5237,  0.3512],\n",
      "        [-0.1329, -0.1158],\n",
      "        [-0.1646,  0.0527],\n",
      "        [ 0.0940, -0.0597],\n",
      "        [ 0.2347, -0.0792],\n",
      "        [ 0.3534,  0.0312]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "print(wei)\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "print(\"wei===\")\n",
    "print(wei)\n",
    "print(x.shape)\n",
    "print(wei.shape)\n",
    "%timeit wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "xbow2 = wei @ x\n",
    "print(\"x[2]\")\n",
    "print(x[2])\n",
    "print(\"xbow2===\")\n",
    "print(xbow2[2])\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOURrfG-ysoL",
    "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tril===\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "wei===\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "wei===\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "wei===\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "12.7 µs ± 27.7 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "result===\n",
      "tensor([[-0.9012,  0.5656],\n",
      "        [-0.4882,  0.7507],\n",
      "        [ 0.5893, -0.4552],\n",
      "        [-0.8135,  0.2670],\n",
      "        [-0.5531,  0.6016],\n",
      "        [-0.9271,  0.5655],\n",
      "        [-2.4451, -0.1605],\n",
      "        [ 0.1804,  2.2347]])\n",
      "tensor([[-0.9012,  0.5656],\n",
      "        [-0.6947,  0.6581],\n",
      "        [-0.2667,  0.2870],\n",
      "        [-0.4034,  0.2820],\n",
      "        [-0.4334,  0.3459],\n",
      "        [-0.5157,  0.3825],\n",
      "        [-0.7913,  0.3050],\n",
      "        [-0.6698,  0.5462]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "print(\"tril===\")\n",
    "print(tril)\n",
    "wei = torch.zeros((T,T))\n",
    "print(\"wei===\")\n",
    "print(wei)\n",
    "\n",
    "# 将wei中对应tril == 0为True的元素替换为负无穷。也就是说，如果tril的某个元素等于0，那么wei的对应元素就会被替换为负无穷。\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(\"wei===\")\n",
    "print(wei)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(\"wei===\")\n",
    "print(wei)\n",
    "%timeit wei @ x\n",
    "xbow3 = wei @ x\n",
    "print(\"result===\")\n",
    "print(x[0])\n",
    "print(xbow3[0])\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDarxEWIRMKq",
    "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n",
      "torch.Size([4, 8, 16])\n",
      "========\n",
      "torch.Size([4, 8, 8])\n",
      "torch.Size([4, 8, 16])\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(23)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "# 定义了一个线性变换层，它接收一个形状为(N, C)的输入，返回一个形状为(N, head_size)的输出。这个线性变换层的权重矩阵的形状是(C, head_size)，它没有偏置向量（bias vector），因为bias=False。\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "print(k.shape)\n",
    "print(q.shape)\n",
    "print(\"========\")\n",
    "print(wei.shape)\n",
    "print(v.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT1hdtzXCjgL",
    "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8062, 0.1938, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1200, 0.3400, 0.5400, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0527, 0.1377, 0.7619, 0.0477, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0753, 0.0918, 0.2166, 0.1137, 0.5027, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0150, 0.1046, 0.4703, 0.1484, 0.0923, 0.1694, 0.0000, 0.0000],\n",
       "        [0.3479, 0.3028, 0.0839, 0.0663, 0.0802, 0.0620, 0.0569, 0.0000],\n",
       "        [0.1496, 0.0608, 0.0529, 0.2932, 0.0411, 0.2122, 0.1015, 0.0886]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5CvobiQ0pLr"
   },
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4SNbLq5z3oBw"
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "wei2 = q @ k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl6I9n9IRTSo",
    "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9552)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1tQx7oeRvtc",
    "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9926)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLb_odHU3iKM",
    "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0389)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.6229)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei2.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB82yzt44REI",
    "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mpt8569BB9_f",
    "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "dRJH6wM_XFfU"
   },
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcvKeBXoZFOY"
   },
   "source": [
    "### Full finished code, for reference\n",
    "\n",
    "You may want to refer directly to the git repo instead though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.824938 M parameters\n",
      "step 0: train loss 9.5725, val loss 9.5551, time 0.12ms\n",
      "step 100: train loss 6.4536, val loss 6.2522, time 20.93ms\n",
      "step 200: train loss 6.3878, val loss 6.1664, time 20.98ms\n",
      "step 300: train loss 6.2587, val loss 6.1671, time 20.96ms\n",
      "step 400: train loss 6.1392, val loss 6.1886, time 21.17ms\n",
      "step 500: train loss 6.0840, val loss 6.1790, time 20.96ms\n",
      "step 600: train loss 6.0741, val loss 6.1969, time 20.96ms\n",
      "step 700: train loss 6.0468, val loss 6.1890, time 20.98ms\n",
      "step 800: train loss 6.0393, val loss 6.1778, time 20.99ms\n",
      "step 900: train loss 6.0409, val loss 6.1480, time 21.11ms\n",
      "step 1000: train loss 6.0405, val loss 6.1822, time 20.94ms\n",
      "step 1100: train loss 6.0093, val loss 6.1803, time 20.94ms\n",
      "step 1200: train loss 5.9921, val loss 6.1816, time 20.94ms\n",
      "step 1300: train loss 5.9906, val loss 6.1237, time 21.01ms\n",
      "step 1400: train loss 5.9865, val loss 6.1327, time 21.09ms\n",
      "step 1500: train loss 5.9950, val loss 6.1291, time 21.02ms\n",
      "step 1600: train loss 5.9614, val loss 6.0635, time 21.28ms\n",
      "step 1700: train loss 5.9525, val loss 6.1307, time 20.98ms\n",
      "step 1800: train loss 5.9394, val loss 6.1067, time 21.18ms\n",
      "step 1900: train loss 5.9170, val loss 6.0589, time 21.16ms\n",
      "step 2000: train loss 5.9151, val loss 6.0855, time 21.11ms\n",
      "step 2100: train loss 5.8798, val loss 6.0659, time 21.15ms\n",
      "step 2200: train loss 5.8664, val loss 6.0816, time 17.84ms\n",
      "step 2300: train loss 5.8622, val loss 6.0125, time 22.36ms\n",
      "step 2400: train loss 5.8346, val loss 6.0418, time 22.48ms\n",
      "step 2500: train loss 5.7952, val loss 6.0811, time 22.30ms\n",
      "step 2600: train loss 5.8002, val loss 6.0267, time 22.26ms\n",
      "step 2700: train loss 5.7644, val loss 5.9990, time 22.49ms\n",
      "step 2800: train loss 5.7690, val loss 5.9822, time 22.29ms\n",
      "step 2900: train loss 5.7515, val loss 5.9248, time 22.48ms\n",
      "step 3000: train loss 5.7148, val loss 5.9466, time 22.54ms\n",
      "step 3100: train loss 5.7037, val loss 5.9116, time 22.23ms\n",
      "step 3200: train loss 5.6796, val loss 5.9277, time 22.71ms\n",
      "step 3300: train loss 5.6854, val loss 5.9096, time 21.18ms\n",
      "step 3400: train loss 5.6637, val loss 5.9218, time 21.73ms\n",
      "step 3500: train loss 5.6582, val loss 5.8725, time 18.83ms\n",
      "step 3600: train loss 5.6573, val loss 5.8726, time 18.88ms\n",
      "step 3700: train loss 5.6319, val loss 5.8568, time 22.35ms\n",
      "step 3800: train loss 5.6199, val loss 5.8709, time 22.28ms\n",
      "step 3900: train loss 5.6160, val loss 5.8476, time 22.48ms\n",
      "step 4000: train loss 5.6003, val loss 5.8159, time 22.24ms\n",
      "step 4100: train loss 5.5889, val loss 5.7909, time 22.43ms\n",
      "step 4200: train loss 5.5719, val loss 5.7885, time 22.34ms\n",
      "step 4300: train loss 5.5841, val loss 5.8158, time 22.89ms\n",
      "step 4400: train loss 5.5647, val loss 5.7934, time 22.26ms\n",
      "step 4500: train loss 5.5270, val loss 5.7699, time 22.23ms\n",
      "step 4600: train loss 5.5343, val loss 5.7706, time 22.25ms\n",
      "step 4700: train loss 5.5231, val loss 5.7657, time 22.49ms\n",
      "step 4800: train loss 5.5295, val loss 5.7474, time 22.63ms\n",
      "step 4900: train loss 5.5202, val loss 5.7159, time 22.56ms\n",
      "step 4999: train loss 5.4942, val loss 5.7297, time 22.83ms\n",
      "\n",
      "齐灌火进双清出，花骨留漾唤招过。\n",
      "几年礼实曾相睡，且以渐离花，啄窗利十六月明月和。\n",
      "谁谓枥丘此锦后，胜道俸地西阳郎。\n",
      "甘泉庭明归寻远，依然老屿初}香通中。\n",
      "领官坐买蓑，瓦团梗氏周苍紫。\n",
      "固作绛海妍儿童，五缯老蛟依前埃。\n",
      "不矜义地昃不用，几亭离怀在宿谒。\n",
      "大事人言无限余，萧柱岩花当已留。\n",
      "归来相伴索坐听，茗装方为几几愚。\n",
      "天晓云日如囚落，自似区区气我魂。\n",
      "在显光辉野连烟，戟竹妙庭欲孤𥊚。\n",
      "醉潮饥寺旧定数，一为过眼应无计。\n",
      "盈愤逢门之亦继，更与当年得身招。\n",
      "拽禄芳公深景相，往事轩长润可存。\n",
      "都已便乐懒踏己，金然煅裳去身开。\n",
      "阳桥帝得皇佛伏，丹陵牧僧弄昭衰。\n",
      "借期忠何奈桃京，新节风前卷微尼。\n",
      "老声学欲同临戚，显酒两相逢文城。\n",
      "田外境辰江淮外，更赠明月离无留。\n",
      "黄云旧游戎可事，几理天涯犹衷云。\n",
      "充舆且得幽河虚，得兮可怜百丈人。\n",
      "倒旦有樵云气襟，棋奴细逐天小械。\n",
      "叙将天我临清攀，行世亿叶老故乡。\n",
      "起来能、直自殿，莺迹槎衣契一鸣。\n",
      "不合一物不是鱼，擘红出手总蒙花。\n",
      "我非心活锦武兼，洗客云峩进履情。\n",
      "更趋西施当理，傲谷舞箭炼粢差挽。\n",
      "孤潢伤城龙面婉，所畏长声万锺签。\n",
      "此郎哀门过出眠，断缸如此自翦垂。\n",
      "空偈几庙堤，初领到今尤多泬。\n",
      "相容春芳夹，重孤心木那。\n",
      "凄凉夜独萱，醉唇无有搔。\n",
      "松水野吟浸，古人双腊外。\n",
      "苍密路如抵，光阴皱鱼飞。\n",
      "挥蹀青玉水，清日客纽衣。\n",
      "故人不好病，五万压汝资。\n",
      "不看欹迹扫，床史下山晴。\n",
      "涨子并临天，世山护窗开。\n",
      "杯中倍外去，左香秋还寻。\n",
      "邻辱不干梦，二影塔后交。\n",
      "忘日蔽处福，作马时使引。\n",
      "日行茅，蛟堂去穿浓，露上簪龙干聿昏。\n",
      "同汝旧日不为昌，碧马今猎看容望。\n",
      "菁日帆际缕纱，采酊绝筵深已未生。\n",
      "小城天子宣，旷与塑交良。\n",
      "）贪来，武门初却茸不受。\n",
      "平生丹马，偶见鸟深。\n",
      "天垂扶，灵酒宁犹持隘镒。\n",
      "知云作羮地，暴古忘情屯。\n",
      "名者不教过，又静雨潺湲。\n",
      "旋贡笼函然，从得编津山。\n",
      "何须悲汉拙，言士为矛名。\n",
      "不见了书厄，教者蟠清麟。\n",
      "寄惭个我卖，毘怪独相少。\n",
      "身虚仙峰寺，烹里焚澄翁。\n",
      "极人同贵事，翻赋子句本。\n",
      "髣集忘垂云，断肠明明气。\n",
      "吾曹虽门种，重没饭颜定。\n",
      "紫杨一笑势，后道变孤另。\n",
      "小翁正无言，朗听炽世蕃。\n",
      "又待穰吾处，叠莎桐旌籁。\n",
      "只将强兄抵，昔恐宇马蹄。\n",
      "尧生饮无处，影缨交妓低。\n",
      "及胥何之著，羞得漆而善。\n",
      "老节迁诉爱，棱兴山林犹。\n",
      "恨宾久在党，可在仍失厚。\n",
      "扫人频当住，带烟毒火可。\n",
      "垂豆怀味影，舞诛无又知。\n",
      "隔草和的违，寝迹有何暇。\n",
      "杜门五点英，天外入风堆。\n",
      "已身是辙殿眼，向不见空翠蔡岩融。\n",
      "月明江南军绿障，庭麟风离入岳漫。\n",
      "一醉最后不如淳，潜一愁不岁不天。\n",
      "两时啄处当年论，或是沈翼雪点绵。\n",
      "微云夜筇宫牛色，水望浄无墙蛮香。\n",
      "翠微少丛尚为尽，摸余朱虹已鹿平。\n",
      "昭夕担，身因惜手倚。\n",
      "几人占头间，风来等可喜。\n",
      "方生常无才，勗修春成寺。\n",
      "微簷晨影暗，伤公常春泉。\n",
      "樵概竹竹谈，蓬陵经日月。\n",
      "松巳不见塔，岸香戎幞漠。\n",
      "设作水经日，帽怪珠戟喉。\n",
      "人意信人业，朱阃珠丛窗。\n",
      "碧逖无数鉴，独其泡山河。\n",
      "闲师将三月，不辜丹鶵秦。\n",
      "欲忘月舌，又临安用嵇谩。\n",
      "莫复米果人，则即如道休。\n",
      "溪汉老矣往，三十国岂东。\n",
      "安不忍以属，人心悔未复。\n",
      "溪云生气动，新驱江渡油。\n",
      "矛娟不可恋，富贵悲双鲤。\n",
      "正荀寻后矣，兀寻力余春。\n",
      "羞人固遭健，一日潘环渌。\n",
      "斗鼎来往事，温叶邀遗辱。\n",
      "怀是永歌角，知醉何所天。\n",
      "多睫安正刀，永鸟为中祠。\n",
      "百折健此古，朝日待紫霖。\n",
      "万江计寻乐，冠垣钟鸡昏。\n",
      "井外潮龄小，荐大论平生。\n",
      "正期先忘恣，萧徽短路遇。\n",
      "曲渌俱欲奇雨结，谁怜天者重天莫。\n",
      "朔风深如汀里雨，漾谈误张归莫违。\n",
      "诵言盛载行分遁，答送处养新轻弦。\n",
      "路肇万日犹应来，岂无梅窦雨已品。\n",
      "此心山左迫可夸，苔窟向今争心通。\n",
      "百丈雷骑楚旌史，岂能尝逢如君霁。\n",
      "叔源结心争散质，筥复千五穷中雉。\n",
      "洒汉分尺园，谈辔迎度莫池秋。\n",
      "看壑从朝腾。\n",
      "祇童宴色上泪裘，万年真默空汗池。\n",
      "洞庭每暖石筋道，大洋唤人爱天根。\n",
      "蛩唱不须举苟用，便苕壶棱亦争阶。\n",
      "何乎回妖相短看，竞机四海晦后哀。\n",
      "相见满道最比白，班似万里补应醒。\n",
      "老去远夜忘早关，揽坛读穰首秦逵。\n",
      "冉蔼种旧中醉粟，梅盟共故知扬名。\n",
      "溪红风紫透愁远，褐襦欲过恋枕金。\n",
      "苍霞水下溪瘴雨，万里流樯渡疑田。\n",
      "金碧雨满盈黄长，溪寒红松涨浪风。\n",
      "一枝不足提节鞅，衣妆玉操熨青春。\n",
      "宿雨满服生啭阔，还向溪山苦去手。\n",
      "此夜雄，翦遍皆凰露。\n",
      "京郭疑愧数余间，个四粤艰精还僧。\n",
      "开薄万营声多雨，清床蛬盈呈张流。\n",
      "道回遥不思只起，自作马床是至期。\n",
      "国从勃宸牡一况，低心无由兮，归眷溪邑大香辙。\n",
      "半声水，邯起盲氲山。\n",
      "腰袍兰虎鬓，退砌情三炼。\n",
      "失君王节思，怆凶犊山真。\n",
      "孤山若荒蛰，本见乡欷住。\n",
      "草木两从此，林携片清妍。\n",
      "怡然闭潮隐，君不自远辈。\n",
      "岸云岂知神，风猿追人赏。\n",
      "照守风骚颦，能应\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 超参数设置\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(23)\n",
    "\n",
    "# 1. 数据处理\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('combined_poetry.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# 2. 数据集划分\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# 3. 数据分批\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# 4. 模型评估\n",
    "# 评估模型在训练集和验证集上的平均损失。这个函数的实现非常简单：它首先将模型设置为评估模式，然后对每个数据集进行eval_iters次迭代。\n",
    "# 在每次迭代中，它获取一个小批量数据，然后计算模型的输出和损失。最后，它返回每个数据集的平均损失。\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# 5. 模型head定义\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # 创建一个下三角矩阵，并将其注册为模型的一个缓冲区。\n",
    "        # 这个下三角矩阵将被用作self-attention的权重矩阵，它将确保模型只能在当前时间步之前的时间步上进行自注意力操作。\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "# 6. 模型MultiHeadAttention定义\n",
    "# 实现多头自注意力机制。在这个机制中，我们并行地进行多次自注意力计算，然后将结果拼接起来，通过一个线性层和一个dropout层进行处理，得到最终的输出。\n",
    "# 这种方法可以让模型在不同的表示子空间中学习输入的不同特征。\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    # __init__方法是类的构造函数，它接收两个参数：num_heads和head_size。num_heads是注意力头的数量，head_size是每个注意力头的大小。\n",
    "    # 创建了一个nn.ModuleList，它包含了num_heads个Head对象。我们还定义了一个线性层self.proj和一个dropout层self.dropout。\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # forward方法定义了前向传播的计算过程。首先，我们对每个注意力头h进行计算，然后将结果在最后一个维度上拼接起来，得到out。\n",
    "    # 然后，我们将out输入到线性层和dropout层，得到最终的输出。\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "# 7. 模型FeedFoward定义\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    # 在__init__方法中，我们首先调用了父类的构造函数，然后定义了一个神经网络self.net。这个神经网络是一个nn.Sequential对象，\n",
    "    # 它包含了两个线性层和一个ReLU激活函数，以及一个dropout层。\n",
    "    # 第一个线性层将输入的维度扩大到4 * n_embd，然后通过ReLU激活函数进行非线性变换，然后第二个线性层将维度缩小回n_embd，最后通过dropout层进行正则化。\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 8. 模型Block定义，layerNorm, multiheadattention, layerNorm, feedforward\n",
    "# Block类的作用是实现一个Transformer模型中的一个块。这个块包含了一个多头自注意力模块和一个前馈神经网络模块，以及两个层归一化操作。\n",
    "# 这种结构可以让模型在处理序列数据时，能够同时考虑到每个位置的信息和全局的信息。\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    # forward方法定义了前向传播的计算过程。首先，我们将输入x进行层归一化，然后输入到自注意力模块中，得到的输出与原始的x相加，得到新的x。\n",
    "    # 然后，我们将新的x进行层归一化，然后输入到前馈神经网络中，得到的输出与原始的x相加，得到最终的输出。\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# 9. 模型NanoLanguageModel定义\n",
    "class NanoLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    # forward方法定义了前向传播的计算过程。首先，我们从词嵌入表和位置嵌入表中获取嵌入，将它们相加得到x。\n",
    "    # 然后，我们将x输入到self.blocks中，进行层归一化，然后输入到self.lm_head中，得到logits。如果提供了目标，我们会计算交叉熵损失。\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # 将当前的索引裁剪到最后的block_size个令牌，获取预测的logits，只关注最后一个时间步，应用softmax得到概率，从分布中采样，将采样的索引添加到运行的序列中。\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "# 10. 模型实例化\n",
    "model = NanoLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "# 将其移动到设备device上。然后，我们打印出模型中的参数数量。\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "t0 = time.time()\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time {dt*1000:.2f}ms\")\n",
    "\n",
    "    # 从训练集中采样一个批次的数据xb和yb，将它们输入到模型中，得到logits和loss。然后将优化器的梯度清零，计算损失的反向传播，更新优化器的参数。\n",
    "    # 采样一个批次的数据，计算损失，清零梯度，计算反向传播，然后更新参数。这是训练神经网络模型的基本步骤。\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    # 清零优化器的梯度。这是因为PyTorch的优化器在每次更新参数时都会累积梯度，所以在每次更新参数之前，我们需要清零梯度。\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # 计算损失的反向传播。这会计算出每个参数的梯度。\n",
    "    loss.backward()\n",
    "    # 更新优化器的参数。这会根据每个参数的梯度和学习率来更新参数的值。\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "# 用模型来生成新的文本，首先创建一个全零的上下文context，将其输入到模型的generate方法中，得到生成的文本，然后将其解码并打印出来。\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
